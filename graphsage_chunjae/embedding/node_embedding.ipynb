{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mchapter = pd.read_csv(\"mchp_relation.csv\")\n",
    "\n",
    "\n",
    "mchapter = mchapter.drop(columns=['성취기준코드', '성취수준 A','성취수준 B','성취수준 C','f_schapter_id','f_schapter_nm','f_tchapter_id','f_tchapter_nm','핵심키워드_v1','성취기준 내용'])\n",
    "\n",
    "mchapter = mchapter.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 저장 완료: mchapter_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# KoBERT 모델과 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"텍스트를 KoBERT 임베딩으로 변환\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embedding = outputs.pooler_output  # [CLS] 임베딩\n",
    "    return cls_embedding\n",
    "\n",
    "def process_unit(title, keywords):\n",
    "    \"\"\"\n",
    "    제목과 키워드를 각각 임베딩하고 결합\n",
    "    - title: 제목 (문자열)\n",
    "    - keywords: 쉼표로 구분된 핵심 키워드 문자열\n",
    "    \"\"\"\n",
    "    # 제목 임베딩\n",
    "    title_embedding = get_embedding(title)\n",
    "\n",
    "    # 키워드 임베딩\n",
    "    keyword_list = [kw.strip() for kw in keywords.split(',')] if keywords else []\n",
    "    keyword_embeddings = []\n",
    "    for keyword in keyword_list:\n",
    "        keyword_embeddings.append(get_embedding(keyword))\n",
    "\n",
    "    # 키워드 평균 임베딩\n",
    "    if keyword_embeddings:\n",
    "        keyword_embeddings = torch.stack(keyword_embeddings)  # (키워드 수, 768)\n",
    "        keyword_mean_embedding = keyword_embeddings.mean(dim=0)  # (1, 768)\n",
    "    else:\n",
    "        keyword_mean_embedding = torch.zeros_like(title_embedding)\n",
    "\n",
    "    # 제목과 키워드 임베딩 결합\n",
    "    combined_embedding = torch.cat([title_embedding, keyword_mean_embedding], dim=1)  # (1, 1536)\n",
    "\n",
    "    return combined_embedding\n",
    "\n",
    "# CSV 파일 읽기\n",
    "file_path = \"mchaper_for_embedding.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 결과 저장 딕셔너리\n",
    "embedding_dict = {}\n",
    "\n",
    "# 데이터프레임 순회하며 임베딩 생성\n",
    "for _, row in df.iterrows():\n",
    "    chapter_id = row['f_mchapter_id']  # ID 컬럼\n",
    "    title = row['f_mchapter_nm']  # 제목 컬럼\n",
    "    keywords = row['핵심키워드_v2']  # 핵심키워드 컬럼\n",
    "    combined_embedding = process_unit(title, keywords)\n",
    "    embedding_dict[chapter_id] = combined_embedding.squeeze(0).numpy()  # numpy 배열로 저장\n",
    "\n",
    "# 임베딩 결과를 .npy 파일로 저장\n",
    "output_file = \"mchapter_embeddings.npy\"\n",
    "np.save(output_file, embedding_dict)\n",
    "\n",
    "print(f\"임베딩 저장 완료: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 임베딩 개수: 94\n",
      "ID 14201907의 임베딩 크기: (1536,)\n",
      "ID 14201907의 임베딩 값:\n",
      "[ 0.02019288 -0.04241268 -0.26368633 ...  0.07522373 -0.06390283\n",
      "  0.03241761]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 저장된 .npy 파일 불러오기\n",
    "file_path = \"mchapter_embeddings.npy\"  # 저장된 파일 경로\n",
    "embedding_dict = np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"총 임베딩 개수: {len(embedding_dict)}\")\n",
    "\n",
    "# 특정 f_mchapter_id의 임베딩 가져오기\n",
    "chapter_id = 14201907  # 예: f_mchapter_id가 1인 경우\n",
    "embedding = embedding_dict.get(chapter_id, None)\n",
    "\n",
    "if embedding is not None:\n",
    "    print(f\"ID {chapter_id}의 임베딩 크기: {embedding.shape}\")\n",
    "    print(f\"ID {chapter_id}의 임베딩 값:\\n{embedding}\")\n",
    "else:\n",
    "    print(f\"ID {chapter_id}에 해당하는 임베딩이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 축소된 임베딩 배열 크기: (94, 94)\n",
      "차원 축소된 94차원 임베딩을 reduced_mchapter_embeddings_94.npy에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1536차 -> 94차원으로 축소\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 딕셔너리 로드\n",
    "embeddings_dict = np.load(\"mchapter_embeddings.npy\", allow_pickle=True).item()\n",
    "\n",
    "# 키와 값을 분리\n",
    "f_mchapter_ids = list(embeddings_dict.keys())\n",
    "embeddings = np.array(list(embeddings_dict.values()))\n",
    "\n",
    "# PCA로 94차원으로 차원 축소\n",
    "pca = PCA(n_components=94)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"차원 축소된 임베딩 배열 크기: {reduced_embeddings.shape}\")  # (94, 94)\n",
    "\n",
    "# 결과 저장\n",
    "output_file = \"reduced_mchapter_embeddings_94.npy\"\n",
    "np.save(output_file, {\"ids\": f_mchapter_ids, \"embeddings\": reduced_embeddings})\n",
    "print(f\"차원 축소된 94차원 임베딩을 {output_file}에 저장했습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 데이터 타입: <class 'dict'>\n",
      "데이터의 키: ['ids', 'embeddings']\n",
      "첫 번째 키의 데이터: [14201779, 14201780, 14201781, 14201782, 14201783, 14201784, 14201785, 14201786, 14201787, 14201788, 14201789, 14201790, 14201791, 14201792, 14201793, 14201794, 14201795, 14201796, 14201797, 14201798, 14201799, 14201800, 14201801, 14201802, 14201803, 14201804, 14201805, 14201806, 14201807, 14201808, 14201809, 14201810, 14201811, 14201812, 14201813, 14201814, 14201815, 14201816, 14201817, 14201818, 14201819, 14201820, 14201821, 14201857, 14201858, 14201859, 14201860, 14201861, 14201862, 14201863, 14201864, 14201865, 14201866, 14201867, 14201868, 14201869, 14201870, 14201871, 14201872, 14201873, 14201874, 14201875, 14201876, 14201877, 14201878, 14201879, 14201880, 14201881, 14201882, 14201883, 14201884, 14201885, 14201886, 14201887, 14201888, 14201889, 14201890, 14201891, 14201892, 14201893, 14201894, 14201895, 14201896, 14201897, 14201898, 14201899, 14201900, 14201901, 14201902, 14201903, 14201904, 14201905, 14201906, 14201907]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# npy 파일 로드\n",
    "file_path = \"reduced_mchapter_embeddings_94.npy\"  # 파일 경로를 적절히 수정하세요\n",
    "node_features = np.load(file_path, allow_pickle=True).item()  # .item()으로 객체 추출\n",
    "\n",
    "# 데이터 구조와 일부 데이터 출력\n",
    "print(\"파일 데이터 타입:\", type(node_features))  # 데이터 타입 확인\n",
    "print(\"데이터의 키:\", list(node_features.keys())[:5])  # 딕셔너리 키 확인\n",
    "print(\"첫 번째 키의 데이터:\", node_features[next(iter(node_features))])  # 첫 번째 값 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 데이터 타입: <class 'dict'>\n",
      "데이터의 키: ['ids', 'embeddings']\n",
      "\n",
      "첫 5개의 키와 데이터:\n",
      "ID ids의 임베딩: [14201779, 14201780, 14201781, 14201782, 14201783, 14201784, 14201785, 14201786, 14201787, 14201788, 14201789, 14201790, 14201791, 14201792, 14201793, 14201794, 14201795, 14201796, 14201797, 14201798, 14201799, 14201800, 14201801, 14201802, 14201803, 14201804, 14201805, 14201806, 14201807, 14201808, 14201809, 14201810, 14201811, 14201812, 14201813, 14201814, 14201815, 14201816, 14201817, 14201818, 14201819, 14201820, 14201821, 14201857, 14201858, 14201859, 14201860, 14201861, 14201862, 14201863, 14201864, 14201865, 14201866, 14201867, 14201868, 14201869, 14201870, 14201871, 14201872, 14201873, 14201874, 14201875, 14201876, 14201877, 14201878, 14201879, 14201880, 14201881, 14201882, 14201883, 14201884, 14201885, 14201886, 14201887, 14201888, 14201889, 14201890, 14201891, 14201892, 14201893, 14201894, 14201895, 14201896, 14201897, 14201898, 14201899, 14201900, 14201901, 14201902, 14201903, 14201904, 14201905, 14201906, 14201907]\n",
      "ID embeddings의 임베딩: [[ 6.2419691e+00 -1.6474595e+00 -7.2047752e-01 ...  5.3601542e-12\n",
      "   4.3655392e-12  2.2399736e-12]\n",
      " [ 2.6938226e+00 -1.9101764e+00 -1.3908223e+00 ...  5.1833594e-12\n",
      "   4.0729620e-12  1.4286630e-12]\n",
      " [-2.6737387e+00 -3.2128057e+00  4.5387229e-01 ... -1.1689260e-07\n",
      "   3.3283765e-08  8.8579995e-08]\n",
      " ...\n",
      " [-2.4419453e+00  2.7671294e+00  1.5589218e+00 ...  1.2847903e-09\n",
      "   9.1313047e-11 -1.0695751e-08]\n",
      " [-2.4601879e+00  2.4073300e+00  1.1727467e+00 ...  5.4476970e-12\n",
      "   4.2073220e-12  1.8215169e-12]\n",
      " [ 1.5373060e-01  2.5913162e+00 -1.1643054e+00 ...  5.6505976e-12\n",
      "   4.5327288e-12  1.6245962e-12]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# npy 파일 로드\n",
    "file_path = \"reduced_mchapter_embeddings_94.npy\"  # 파일 경로를 적절히 수정하세요\n",
    "node_features = np.load(file_path, allow_pickle=True).item()  # .item()으로 객체 추출\n",
    "\n",
    "# 데이터 구조와 일부 데이터 출력\n",
    "print(\"파일 데이터 타입:\", type(node_features))  # 데이터 타입 확인\n",
    "print(\"데이터의 키:\", list(node_features.keys())[:5])  # 딕셔너리 키 확인\n",
    "\n",
    "# 첫 5개의 키와 해당 값 출력\n",
    "print(\"\\n첫 5개의 키와 데이터:\")\n",
    "for key in list(node_features.keys())[:5]:\n",
    "    print(f\"ID {key}의 임베딩: {node_features[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 데이터 타입: <class 'dict'>\n",
      "데이터의 키: ['ids', 'embeddings']\n",
      "첫 번째 키: ids\n",
      "임베딩 차원: 94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# npy 파일 로드\n",
    "file_path = \"reduced_mchapter_embeddings_94.npy\"  # 파일 경로를 적절히 수정하세요\n",
    "node_features = np.load(file_path, allow_pickle=True).item()  # .item()으로 객체 추출\n",
    "\n",
    "# 데이터 구조와 일부 데이터 출력\n",
    "print(\"파일 데이터 타입:\", type(node_features))  # 데이터 타입 확인\n",
    "print(\"데이터의 키:\", list(node_features.keys())[:5])  # 딕셔너리 키 확인\n",
    "\n",
    "# 첫 번째 키의 임베딩 확인 및 차원 출력\n",
    "first_key = next(iter(node_features))  # 첫 번째 키\n",
    "embedding = node_features[first_key]  # 해당 키의 임베딩\n",
    "print(f\"첫 번째 키: {first_key}\")\n",
    "print(f\"임베딩 차원: {len(embedding)}\")  # 리스트 길이를 사용해 임베딩 차원 확인\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
