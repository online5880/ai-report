{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 핵심키워드_v2 기존 데이터와 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 성취기준 데이터 로드\n",
    "achievement_data = pd.read_excel('merged_final_data.xlsx')  # 성취기준 데이터 파일\n",
    "keywords_data = pd.read_excel('성취기준.xlsx')  # 핵심키워드_v2 파일\n",
    "\n",
    "# 성취기준코드를 기준으로 병합\n",
    "merged_data = pd.merge(achievement_data, keywords_data, on='성취기준코드', how='left')\n",
    "\n",
    "\n",
    "# 결과 저장\n",
    "merged_data.to_excel('merged_final_data_new.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑셀파일 => csv 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엑셀 파일이 'merged_final_data_new - 복사본.csv'로 성공적으로 변환되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "excel_file = 'merged_final_data_new - 복사본.xlsx'  # 엑셀 파일 이름 또는 경로\n",
    "csv_file = 'merged_final_data_new - 복사본.csv'  # 변환할 CSV 파일 이름\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"엑셀 파일이 '{csv_file}'로 성공적으로 변환되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML 파일이 'new_knowledge_graph_v2.html'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 데이터 로드\n",
    "file_name = \"merged_final_data_new - 복사본.csv\"  \n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# 필수 컬럼 확인\n",
    "required_columns = [\n",
    "    \"f_mchapter_nm\", \"f_mchapter_id\", \"성취기준 내용\", \"성취기준코드\", \"핵심키워드_v2\",\n",
    "    \"성취수준 A\", \"성취수준 B\", \"성취수준 C\", \"f_schapter_id\", \"f_schapter_nm\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"필수 컬럼 '{col}'이(가) 데이터에 없습니다.\")\n",
    "\n",
    "# 중복된 텍스트 제거\n",
    "data = data.drop_duplicates(subset=[\"f_mchapter_nm\", \"성취기준 내용\", \"핵심키워드_v2\"])\n",
    "\n",
    "# 2. 데이터 결합 및 전처리\n",
    "# 텍스트 정제\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"f_mchapter_nm\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"성취기준 내용\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"핵심키워드_v2\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True)\n",
    ")\n",
    "data[\"tooltip\"] = (\n",
    "    \"소단원명: \" + data[\"f_schapter_nm\"].fillna(\"\") +\n",
    "    \"<br>성취수준 A: \" + data[\"성취수준 A\"].fillna(\"\") +\n",
    "    \"<br>성취수준 B: \" + data[\"성취수준 B\"].fillna(\"\") +\n",
    "    \"<br>성취수준 C: \" + data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# 3. 유사도 계산\n",
    "# Sentence-BERT 유사도\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 최종 유사도\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 4. 그래프 생성\n",
    "threshold = 0.8\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "for _, row in data.iterrows():\n",
    "    level_color = \"red\" if pd.notna(row[\"성취수준 A\"]) else \"orange\" if pd.notna(row[\"성취수준 B\"]) else \"yellow\"\n",
    "    graph.add_node(\n",
    "        str(row[\"f_mchapter_id\"]),\n",
    "        label=row[\"f_mchapter_nm\"],\n",
    "        tooltip=row[\"tooltip\"],\n",
    "        color=level_color,\n",
    "        size=20 if pd.notna(row[\"성취수준 A\"]) else 15 if pd.notna(row[\"성취수준 B\"]) else 10\n",
    "    )\n",
    "\n",
    "# 소단원 노드 추가\n",
    "for _, row in data.iterrows():\n",
    "    if pd.notna(row[\"f_schapter_nm\"]):\n",
    "        graph.add_node(\n",
    "            str(row[\"f_schapter_id\"]),\n",
    "            label=row[\"f_schapter_nm\"],\n",
    "            tooltip=\"소단원\",\n",
    "            color=\"lightblue\",\n",
    "            size=10\n",
    "        )\n",
    "        # 상위 단원과 연결\n",
    "        graph.add_edge(\n",
    "            str(row[\"f_mchapter_id\"]),\n",
    "            str(row[\"f_schapter_id\"]),\n",
    "            weight=1,\n",
    "            title=\"소단원 연결\"\n",
    "        )\n",
    "\n",
    "# 엣지 추가 (자기 자신 비교 제외)\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] >= threshold:\n",
    "            graph.add_edge(\n",
    "                str(data.iloc[i][\"f_mchapter_id\"]),\n",
    "                str(data.iloc[j][\"f_mchapter_id\"]),\n",
    "                weight=final_sim[i, j],\n",
    "                title=f\"유사도: {final_sim[i, j]:.2f}\"\n",
    "            )\n",
    "\n",
    "# 5. Pyvis 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -3000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.1\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# HTML 저장\n",
    "output_file = \"new_knowledge_graph_v2.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"HTML 파일이 '{output_file}'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 기반 + 성취기준 기반 선후행 관계 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated graph saved to new_knowledge_graph_v3.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_nm\", \"f_mchapter_id\", \"성취기준코드\", \"성취기준 내용\", \n",
    "    \"성취수준 A\", \"성취수준 B\", \"성취수준 C\", \"f_schapter_id\", \n",
    "    \"f_schapter_nm\", \"핵심키워드_v2\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data = data.drop_duplicates(subset=[\"f_mchapter_nm\", \"성취기준 내용\", \"핵심키워드_v2\"])\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"f_mchapter_nm\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"성취기준 내용\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"핵심키워드_v2\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True)\n",
    ")\n",
    "data[\"tooltip\"] = (\n",
    "    \"소단원명: \" + data[\"f_schapter_nm\"].fillna(\"\") +\n",
    "    \"<br>성취수준 A: \" + data[\"성취수준 A\"].fillna(\"\") +\n",
    "    \"<br>성취수준 B: \" + data[\"성취수준 B\"].fillna(\"\") +\n",
    "    \"<br>성취수준 C: \" + data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Calculate similarities\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(embeddings.cpu(), embeddings.cpu())\n",
    "\n",
    "# Calculate TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "threshold = 0.75\n",
    "\n",
    "# Create the graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 성취기준코드 and 단원\n",
    "for _, row in data.iterrows():\n",
    "    graph.add_node(\n",
    "        row[\"성취기준코드\"],\n",
    "        label=row[\"f_mchapter_nm\"] + \" - \" + row[\"성취기준코드\"],\n",
    "        tooltip=row[\"tooltip\"],\n",
    "        color=\"red\",\n",
    "        size=20\n",
    "    )\n",
    "\n",
    "# Add predefined edges based on 성취기준 선후행 관계\n",
    "predefined_edges = [\n",
    "    (\"2수01-01\", \"2수01-02\"),\n",
    "    (\"2수01-02\", \"2수01-03\"),\n",
    "    (\"2수01-03\", \"2수01-04\"),\n",
    "    (\"2수01-05\", \"2수01-06\"),\n",
    "    (\"2수01-06\", \"2수01-07\"),\n",
    "    (\"2수01-07\", \"2수01-08\"),\n",
    "    (\"2수01-08\", \"2수01-09\"),\n",
    "    (\"2수01-10\", \"2수01-11\"),\n",
    "    (\"2수02-01\", \"2수02-02\"),\n",
    "    (\"2수03-01\", \"2수03-02\"),\n",
    "    (\"2수03-03\", \"2수03-04\"),\n",
    "    (\"2수03-04\", \"2수03-05\"),\n",
    "    (\"2수03-06\", \"2수03-07\"),\n",
    "    (\"2수03-07\", \"2수03-08\"),\n",
    "    (\"2수03-08\", \"2수03-09\"),\n",
    "    (\"2수03-10\", \"2수03-11\"),\n",
    "    (\"2수03-11\", \"2수03-12\"),\n",
    "    (\"2수03-12\", \"2수03-13\"),\n",
    "    (\"2수04-01\", \"2수04-02\"),\n",
    "    (\"2수04-02\", \"2수04-03\")\n",
    "]\n",
    "for edge in predefined_edges:\n",
    "    graph.add_edge(edge[0], edge[1], weight=1.0, title=\"선후행 관계\")\n",
    "\n",
    "# Add similarity-based edges (유사도 기반 연결)\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] >= threshold:\n",
    "            source = data.iloc[i][\"성취기준코드\"]\n",
    "            target = data.iloc[j][\"성취기준코드\"]\n",
    "            if not graph.has_edge(source, target):\n",
    "                graph.add_edge(\n",
    "                    source,\n",
    "                    target,\n",
    "                    weight=final_sim[i, j],\n",
    "                    title=f\"유사도: {final_sim[i, j]:.2f}\",\n",
    "                    color=\"blue\",\n",
    "                    dash=True\n",
    "                )\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.1\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"new_knowledge_graph_v3.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Integrated graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated graph saved to new_knowledge_graph_v4.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_nm\", \"f_mchapter_id\", \"성취기준코드\", \"성취기준 내용\", \n",
    "    \"성취수준 A\", \"성취수준 B\", \"성취수준 C\", \"f_schapter_id\", \n",
    "    \"f_schapter_nm\", \"핵심키워드_v2\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data = data.drop_duplicates(subset=[\"f_mchapter_nm\", \"성취기준 내용\", \"핵심키워드_v2\"])\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"f_mchapter_nm\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"성취기준 내용\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"핵심키워드_v2\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True)\n",
    ")\n",
    "data[\"tooltip\"] = (\n",
    "    \"소단원명: \" + data[\"f_schapter_nm\"].fillna(\"\") +\n",
    "    \"<br>성취수준 A: \" + data[\"성취수준 A\"].fillna(\"\") +\n",
    "    \"<br>성취수준 B: \" + data[\"성취수준 B\"].fillna(\"\") +\n",
    "    \"<br>성취수준 C: \" + data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Calculate similarities\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(embeddings.cpu(), embeddings.cpu())\n",
    "\n",
    "# Calculate TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "threshold = 0.7\n",
    "\n",
    "# Create the graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 성취기준코드 and 단원\n",
    "for _, row in data.iterrows():\n",
    "    graph.add_node(\n",
    "        row[\"성취기준코드\"],\n",
    "        label=row[\"f_mchapter_nm\"] + \" - \" + row[\"성취기준코드\"],\n",
    "        tooltip=row[\"tooltip\"],\n",
    "        color=\"red\",\n",
    "        size=20\n",
    "    )\n",
    "\n",
    "# Add predefined edges based on 성취기준 선후행 관계\n",
    "predefined_edges = [\n",
    "    (\"2수01-01\", \"2수01-02\"),\n",
    "    (\"2수01-02\", \"2수01-03\"),\n",
    "    (\"2수01-03\", \"2수01-04\"),\n",
    "    (\"2수01-05\", \"2수01-06\"),\n",
    "    (\"2수01-06\", \"2수01-07\"),\n",
    "    (\"2수01-07\", \"2수01-08\"),\n",
    "    (\"2수01-08\", \"2수01-09\"),\n",
    "    (\"2수01-10\", \"2수01-11\"),\n",
    "    (\"2수02-01\", \"2수02-02\"),\n",
    "    (\"2수03-01\", \"2수03-02\"),\n",
    "    (\"2수03-03\", \"2수03-04\"),\n",
    "    (\"2수03-04\", \"2수03-05\"),\n",
    "    (\"2수03-06\", \"2수03-07\"),\n",
    "    (\"2수03-07\", \"2수03-08\"),\n",
    "    (\"2수03-08\", \"2수03-09\"),\n",
    "    (\"2수03-10\", \"2수03-11\"),\n",
    "    (\"2수03-11\", \"2수03-12\"),\n",
    "    (\"2수03-12\", \"2수03-13\"),\n",
    "    (\"2수04-01\", \"2수04-02\"),\n",
    "    (\"2수04-02\", \"2수04-03\")\n",
    "]\n",
    "for edge in predefined_edges:\n",
    "    graph.add_edge(edge[0], edge[1], weight=1.0, title=\"선후행 관계\")\n",
    "\n",
    "# Add similarity-based edges (유사도 기반 연결)\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] >= threshold:\n",
    "            source = data.iloc[i][\"성취기준코드\"]\n",
    "            target = data.iloc[j][\"성취기준코드\"]\n",
    "            if not graph.has_edge(source, target):\n",
    "                graph.add_edge(\n",
    "                    source,\n",
    "                    target,\n",
    "                    weight=final_sim[i, j],\n",
    "                    title=f\"유사도: {final_sim[i, j]:.2f}\",\n",
    "                    color=\"blue\",\n",
    "                    dash=True\n",
    "                )\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.1\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"new_knowledge_graph_v4.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Integrated graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2+v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated graph saved to new_knowledge_graph_v5.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_nm\", \"f_mchapter_id\", \"성취기준코드\", \"성취기준 내용\", \n",
    "    \"성취수준 A\", \"성취수준 B\", \"성취수준 C\", \"f_schapter_id\", \n",
    "    \"f_schapter_nm\", \"핵심키워드_v2\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data = data.drop_duplicates(subset=[\"f_mchapter_nm\", \"성취기준 내용\", \"핵심키워드_v2\"])\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"f_mchapter_nm\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"성취기준 내용\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"핵심키워드_v2\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True)\n",
    ")\n",
    "data[\"tooltip\"] = (\n",
    "    \"소단원명: \" + data[\"f_schapter_nm\"].fillna(\"\") +\n",
    "    \"<br>성취수준 A: \" + data[\"성취수준 A\"].fillna(\"\") +\n",
    "    \"<br>성취수준 B: \" + data[\"성취수준 B\"].fillna(\"\") +\n",
    "    \"<br>성취수준 C: \" + data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Calculate similarities\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(embeddings.cpu(), embeddings.cpu()).astype('float64')  # Ensure float64\n",
    "\n",
    "# Combine similarities\n",
    "threshold = 0.85  # Threshold for similarity\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 중단원\n",
    "for _, row in data.iterrows():\n",
    "    graph.add_node(\n",
    "        row[\"성취기준코드\"],\n",
    "        label=row[\"f_mchapter_nm\"] + \" - \" + row[\"성취기준코드\"],\n",
    "        tooltip=row[\"tooltip\"],\n",
    "        color=\"green\",\n",
    "        size=20\n",
    "    )\n",
    "\n",
    "# Add predefined edges based on 성취기준 선후행 관계\n",
    "predefined_edges = [\n",
    "    (\"2수01-01\", \"2수01-02\"),\n",
    "    (\"2수01-02\", \"2수01-03\"),\n",
    "    (\"2수01-03\", \"2수01-04\"),\n",
    "    (\"2수01-05\", \"2수01-06\"),\n",
    "    (\"2수01-06\", \"2수01-07\"),\n",
    "    (\"2수01-07\", \"2수01-08\"),\n",
    "    (\"2수01-08\", \"2수01-09\"),\n",
    "    (\"2수01-10\", \"2수01-11\"),\n",
    "    (\"2수02-01\", \"2수02-02\"),\n",
    "    (\"2수03-01\", \"2수03-02\"),\n",
    "    (\"2수03-03\", \"2수03-04\"),\n",
    "    (\"2수03-04\", \"2수03-05\"),\n",
    "    (\"2수03-06\", \"2수03-07\"),\n",
    "    (\"2수03-07\", \"2수03-08\"),\n",
    "    (\"2수03-08\", \"2수03-09\"),\n",
    "    (\"2수03-10\", \"2수03-11\"),\n",
    "    (\"2수03-11\", \"2수03-12\"),\n",
    "    (\"2수03-12\", \"2수03-13\"),\n",
    "    (\"2수04-01\", \"2수04-02\"),\n",
    "    (\"2수04-02\", \"2수04-03\")\n",
    "]\n",
    "for edge in predefined_edges:\n",
    "    graph.add_edge(edge[0], edge[1], weight=1.0, title=\"선후행 관계\", color=\"blue\")\n",
    "\n",
    "# Add similarity-based edges\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):  # Only forward connections allowed\n",
    "        if bert_sim[i, j] >= threshold:\n",
    "            source = data.iloc[i][\"성취기준코드\"]\n",
    "            target = data.iloc[j][\"성취기준코드\"]\n",
    "            graph.add_edge(\n",
    "                source,\n",
    "                target,\n",
    "                weight=float(bert_sim[i, j]),  # Ensure float is JSON serializable\n",
    "                title=f\"유사도: {bert_sim[i, j]:.2f}\",\n",
    "                color=\"red\"\n",
    "            )\n",
    "\n",
    "# Remove self-loops and duplicated edges\n",
    "edges_to_remove = []\n",
    "for u, v in graph.edges():\n",
    "    if u == v:  # Remove self-loops\n",
    "        edges_to_remove.append((u, v))\n",
    "for edge in edges_to_remove:\n",
    "    graph.remove_edge(*edge)\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -3000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 150,\n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 20\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"new_knowledge_graph_v5.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Integrated graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized graph saved to new_knowledge_graph_v6.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\"f_mchapter_nm\", \"성취기준코드\", \"성취기준 내용\", \"핵심키워드_v2\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Combine text for similarity calculation\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"핵심키워드_v2\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Sort data by 성취기준코드\n",
    "data = data.sort_values(by=\"성취기준코드\").reset_index(drop=True)\n",
    "\n",
    "# Perform similarity-based analysis\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "similarity_matrix = cosine_similarity(embeddings.cpu(), embeddings.cpu())\n",
    "\n",
    "# Generate 성취기준 흐름 기반 edges\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    flow_edges.append((data.iloc[i][\"성취기준코드\"], data.iloc[i + 1][\"성취기준코드\"], \"선후행 관계\"))\n",
    "\n",
    "# Generate 유사도 기반 edges\n",
    "threshold = 0.75  # Set a high threshold for similarity\n",
    "similarity_edges = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):  # Only forward connections allowed\n",
    "        if similarity_matrix[i, j] >= threshold:\n",
    "            similarity_edges.append((data.iloc[i][\"성취기준코드\"], data.iloc[j][\"성취기준코드\"], \"유사도 기반 연결\"))\n",
    "\n",
    "# Combine edges\n",
    "all_edges = flow_edges + similarity_edges\n",
    "\n",
    "# Map 성취기준코드 to 중단원명\n",
    "code_to_chapter = data.set_index(\"성취기준코드\")[\"f_mchapter_nm\"].to_dict()\n",
    "\n",
    "# Create the graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 중단원\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].unique()\n",
    "for chapter in unique_mchapters:\n",
    "    graph.add_node(\n",
    "        chapter,\n",
    "        label=chapter,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# Add edges to the graph\n",
    "for source, target, edge_type in all_edges:\n",
    "    source_chapter = code_to_chapter[source]\n",
    "    target_chapter = code_to_chapter[target]\n",
    "    if source_chapter != target_chapter:  # Avoid self-loops\n",
    "        graph.add_edge(\n",
    "            source_chapter, target_chapter,\n",
    "            color=\"blue\" if edge_type == \"선후행 관계\" else \"red\",\n",
    "            title=edge_type\n",
    "        )\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 150,\n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"new_knowledge_graph_v6.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Optimized graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성취기준 코드 흐름으로만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with chapter connections by 성취기준코드 flow saved to chapter_by_code_flow_graph.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\"f_mchapter_nm\", \"성취기준코드\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess 중단원명\n",
    "data[\"f_mchapter_nm\"] = (\n",
    "    data[\"f_mchapter_nm\"]\n",
    "    .str.strip()  # 공백 제거\n",
    "    .str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)  # 특수문자 제거\n",
    ")\n",
    "\n",
    "# Sort data by 성취기준코드 to establish flow\n",
    "data = data.sort_values(by=\"성취기준코드\").reset_index(drop=True)\n",
    "\n",
    "# Map 성취기준코드 to 중단원명\n",
    "code_to_chapter = data.set_index(\"성취기준코드\")[\"f_mchapter_nm\"].to_dict()\n",
    "\n",
    "# Generate edges based on 성취기준코드 flow\n",
    "edges = []\n",
    "previous_chapter = None\n",
    "for _, row in data.iterrows():\n",
    "    current_chapter = row[\"f_mchapter_nm\"]\n",
    "    if previous_chapter and previous_chapter != current_chapter:\n",
    "        edges.append((previous_chapter, current_chapter))\n",
    "    previous_chapter = current_chapter\n",
    "\n",
    "# Extract unique 중단원명\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].drop_duplicates().tolist()\n",
    "\n",
    "# Create the graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 중단원명\n",
    "for chapter in unique_mchapters:\n",
    "    graph.add_node(\n",
    "        chapter,\n",
    "        label=chapter,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# Add edges for 중단원 선후행 관계\n",
    "for edge in edges:\n",
    "    graph.add_edge(\n",
    "        edge[0], edge[1],\n",
    "        weight=1.0,\n",
    "        color=\"blue\",\n",
    "        title=\"성취기준코드 흐름 기반 중단원 선후행 관계\"\n",
    "    )\n",
    "\n",
    "# Add edges to ensure all nodes are connected if needed\n",
    "connected_components = list(nx.weakly_connected_components(graph))\n",
    "if len(connected_components) > 1:\n",
    "    for i in range(len(connected_components) - 1):\n",
    "        source = list(connected_components[i])[0]\n",
    "        target = list(connected_components[i + 1])[0]\n",
    "        graph.add_edge(\n",
    "            source, target,\n",
    "            weight=0.1,\n",
    "            color=\"gray\",\n",
    "            title=\"추가 연결\"\n",
    "        )\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.1\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"chapter_by_code_flow_graph.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph with chapter connections by 성취기준코드 flow saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with chapter connections by f_mchapter_id saved to chapter_by_code_flow_graph_with_id.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\"f_mchapter_id\", \"f_mchapter_nm\", \"성취기준코드\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess 중단원명\n",
    "data[\"f_mchapter_nm\"] = (\n",
    "    data[\"f_mchapter_nm\"]\n",
    "    .str.strip()  # 공백 제거\n",
    "    .str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)  # 특수문자 제거\n",
    ")\n",
    "\n",
    "# Sort data by 성취기준코드 to establish flow\n",
    "data = data.sort_values(by=\"성취기준코드\").reset_index(drop=True)\n",
    "\n",
    "# Map 성취기준코드 to 중단원 ID and Name\n",
    "code_to_id = data.set_index(\"성취기준코드\")[\"f_mchapter_id\"].to_dict()\n",
    "id_to_name = data.set_index(\"f_mchapter_id\")[\"f_mchapter_nm\"].to_dict()\n",
    "\n",
    "# Generate edges based on 성취기준코드 flow\n",
    "edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# Extract unique 중단원 IDs\n",
    "unique_ids = data[\"f_mchapter_id\"].drop_duplicates().tolist()\n",
    "\n",
    "# Create the graph\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes for 중단원 ID with labels as 중단원명\n",
    "for node_id in unique_ids:\n",
    "    graph.add_node(\n",
    "        node_id,\n",
    "        label=id_to_name[node_id],\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# Add edges for 중단원 선후행 관계\n",
    "for edge in edges:\n",
    "    graph.add_edge(\n",
    "        edge[0], edge[1],\n",
    "        weight=1.0,\n",
    "        color=\"blue\",\n",
    "        title=\"성취기준코드 흐름 기반 중단원 선후행 관계\"\n",
    "    )\n",
    "\n",
    "# Add edges to ensure all nodes are connected if needed\n",
    "connected_components = list(nx.weakly_connected_components(graph))\n",
    "if len(connected_components) > 1:\n",
    "    for i in range(len(connected_components) - 1):\n",
    "        source = list(connected_components[i])[0]\n",
    "        target = list(connected_components[i + 1])[0]\n",
    "        graph.add_edge(\n",
    "            source, target,\n",
    "            weight=0.1,\n",
    "            color=\"gray\",\n",
    "            title=\"추가 연결\"\n",
    "        )\n",
    "\n",
    "# Visualize with PyVis\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Customize physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.1\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"chapter_by_code_flow_graph_with_id.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph with chapter connections by f_mchapter_id saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 코드로 돌린 후, 중단원명이 다 표현되는지 확인하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래프에 포함된 중단원 개수: 94\n",
      "그래프에 포함된 중단원: ['14201779', '14201780', '14201781', '14201782', '14201783', '14201784', '14201785', '14201786', '14201787', '14201788', '14201789', '14201790', '14201791', '14201792', '14201793', '14201794', '14201795', '14201796', '14201797', '14201798', '14201799', '14201800', '14201801', '14201802', '14201803', '14201804', '14201805', '14201806', '14201807', '14201808', '14201809', '14201810', '14201811', '14201812', '14201813', '14201814', '14201815', '14201816', '14201817', '14201818', '14201819', '14201820', '14201821', '14201857', '14201858', '14201859', '14201860', '14201861', '14201862', '14201863', '14201864', '14201865', '14201866', '14201867', '14201868', '14201869', '14201870', '14201871', '14201872', '14201873', '14201874', '14201875', '14201876', '14201877', '14201878', '14201879', '14201880', '14201881', '14201882', '14201883', '14201884', '14201885', '14201886', '14201887', '14201888', '14201889', '14201890', '14201891', '14201892', '14201893', '14201894', '14201895', '14201896', '14201897', '14201898', '14201899', '14201900', '14201901', '14201902', '14201903', '14201904', '14201905', '14201906', '14201907']\n",
      "데이터에서 고유한 중단원 개수: 84\n",
      "데이터에서 고유한 중단원: ['12345알아보기' '6789알아보기' '9까지수의순서' '1만큼더큰수와1만큼더작은수0알아보기' '두수의크기비교'\n",
      " '여러가지모양찾아보기' '여러가지모양알아보기' '2345를모으고가르기' '6789를모으고가르기' '덧셈과뺄셈이야기만들기' '덧셈'\n",
      " '뺄셈' '덧셈과뺄셈' '길이비교하기' '무게비교하기' '넓이비교하기' '담을수있는양비교하기' '높이비교하기' '키비교하기'\n",
      " '10알아보기' '십몇알아보기' '몇십과몇십몇을알아보기' '50까지의수의순서' '백과몇백알아보기' '세자리수알아보기' '뛰어세기'\n",
      " '도형' '쌓기나무' '덧셈과뺄셈의관계' '의값구하기' '세수의계산' '여러가지단위길이로재기' '자로길이재기'\n",
      " '길이를어림해보고재어보기' '분류하기' '묶어세기' '곱셈식알아보기' '곱셈식을활용하기' '몇십알아보기' '99까지의수알아보기'\n",
      " '수의순서알아보기' '두수의크기비교하기' '여러개의수의크기비교하기' '짝수와홀수' '한자리세수의계산' '두수를더하기'\n",
      " '10이되는더하기' '10에서빼기' '10을만들어더하기' '여러가지모양만들기' '몇시알아보기' '몇시30분알아보기'\n",
      " '몇시몇시30분의응용' '10을이용하여모으기와가르기' '받아올림이있는덧셈' '받아내림이있는뺄셈' '규칙찾기' '규칙만들기1'\n",
      " '규칙만들기2' '규칙찾기수' '덧셈하기1' '덧셈하기2' '덧셈하기3' '덧셈하기4' '뺄셈하기1' '뺄셈하기2' '뺄셈하기3'\n",
      " '뺄셈하기4' '1000과몇천알아보기' '네자리수알아보기' '2단3단5단6단곱셈구구' '4단7단8단9단곱셈구구'\n",
      " '1단곱셈구구와0의곱' '곱셈표' '곱셈구구를활용하기' '길이알아보기' '길이의합' '길이의차' '길이어림하기' '시각과시간'\n",
      " '하루의시간알아보기' '달력알아보기' '표로나타내기' '그래프로나타내기']\n",
      "누락된 중단원: {'무게비교하기', '덧셈과뺄셈의관계', '받아내림이있는뺄셈', '두수를더하기', '두수의크기비교', '1단곱셈구구와0의곱', '여러가지모양알아보기', '키비교하기', '4단7단8단9단곱셈구구', '여러가지모양찾아보기', '몇시알아보기', '길이알아보기', '10을만들어더하기', '십몇알아보기', '여러개의수의크기비교하기', '세수의계산', '곱셈식알아보기', '표로나타내기', '덧셈하기4', '곱셈식을활용하기', '하루의시간알아보기', '두수의크기비교하기', '담을수있는양비교하기', '자로길이재기', '규칙만들기1', '분류하기', '2단3단5단6단곱셈구구', '규칙만들기2', '시각과시간', '50까지의수의순서', '묶어세기', '1만큼더큰수와1만큼더작은수0알아보기', '길이의합', '네자리수알아보기', '뺄셈', '길이비교하기', '덧셈하기3', '몇시30분알아보기', '규칙찾기수', '몇십알아보기', '그래프로나타내기', '뺄셈하기1', '넓이비교하기', '몇시몇시30분의응용', '짝수와홀수', '10에서빼기', '수의순서알아보기', '뺄셈하기2', '백과몇백알아보기', '달력알아보기', '한자리세수의계산', '6789알아보기', '곱셈표', '규칙찾기', '덧셈과뺄셈', '여러가지모양만들기', '도형', '의값구하기', '받아올림이있는덧셈', '길이어림하기', '세자리수알아보기', '길이의차', '뛰어세기', '덧셈하기2', '몇십과몇십몇을알아보기', '6789를모으고가르기', '덧셈하기1', '10이되는더하기', '10을이용하여모으기와가르기', '높이비교하기', '99까지의수알아보기', '쌓기나무', '덧셈', '10알아보기', '뺄셈하기4', '12345알아보기', '뺄셈하기3', '곱셈구구를활용하기', '1000과몇천알아보기', '길이를어림해보고재어보기', '덧셈과뺄셈이야기만들기', '9까지수의순서', '여러가지단위길이로재기', '2345를모으고가르기'}\n"
     ]
    }
   ],
   "source": [
    "# 그래프의 노드 이름 확인\n",
    "graph_nodes = [node for node in graph.nodes]\n",
    "print(\"그래프에 포함된 중단원 개수:\", len(graph_nodes))\n",
    "print(\"그래프에 포함된 중단원:\", graph_nodes)\n",
    "\n",
    "# 데이터에서 고유한 중단원 확인\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].unique()\n",
    "print(\"데이터에서 고유한 중단원 개수:\", len(unique_mchapters))\n",
    "print(\"데이터에서 고유한 중단원:\", unique_mchapters)\n",
    "\n",
    "# 누락된 중단원 확인\n",
    "missing_chapters = set(unique_mchapters) - set(graph_nodes)\n",
    "if missing_chapters:\n",
    "    print(\"누락된 중단원:\", missing_chapters)\n",
    "else:\n",
    "    print(\"모든 중단원이 그래프에 포함되어 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-BERT와 TF-IDF 입력 데이터 분리 + 유사도 가장 높은 거 하나만 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with integrated flow and similarity saved to final_chapter_graph.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성\n",
    "flow_edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        flow_edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# 유사도 기반 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "for chapter_id, chapter_name in zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"]):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),  # ID는 문자열로 변환\n",
    "        label=chapter_name,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"blue\",\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true, \n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true, \n",
    "      \"iterations\": 1000, \n",
    "      \"fit\": true\n",
    "    },\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -8000, \n",
    "      \"centralGravity\": 0.3, \n",
    "      \"springLength\": 500, \n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.1\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 20\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"final_chapter_graph.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph with integrated flow and similarity saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-BERT와 TF-IDF 입력 데이터 분리 + 유사도 가장 높은 거 하나만 연결 + 학년학기별 색상 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with integrated flow and similarity saved to final_chapter_graph2.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성\n",
    "flow_edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        flow_edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# 유사도 기반 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 색상 매핑\n",
    "def get_node_color(subject_id):\n",
    "    color_map = {\n",
    "        2212: \"red\",\n",
    "        2213: \"blue\",\n",
    "        2214: \"green\",\n",
    "        2215: \"yellow\"\n",
    "    }\n",
    "    return color_map.get(subject_id, \"gray\")\n",
    "\n",
    "# 노드 추가\n",
    "for chapter_id, chapter_name, subject_id in zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"f_subject_id\"]):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),  # ID는 문자열로 변환\n",
    "        label=chapter_name,\n",
    "        color=get_node_color(subject_id),\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"blue\",\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true, \n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true, \n",
    "      \"iterations\": 1000, \n",
    "      \"fit\": true\n",
    "    },\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -8000, \n",
    "      \"centralGravity\": 0.3, \n",
    "      \"springLength\": 500, \n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.1\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 20\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"final_chapter_graph2.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph with integrated flow and similarity saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final_chapter_graph2 + 색상별 즉 학년 학기별 군집화 = final_chapter_graph3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with integrated flow and similarity saved to final_chapter_graph3.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성\n",
    "flow_edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        flow_edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 색상 및 그룹 설정\n",
    "def get_node_color(subject_id):\n",
    "    color_map = {\n",
    "        2212: \"#FF4444\",  # 더 선명한 빨강\n",
    "        2213: \"#4444FF\",  # 더 선명한 파랑\n",
    "        2214: \"#44FF44\",  # 더 선명한 초록\n",
    "        2215: \"#FFFF44\"   # 더 선명한 노랑\n",
    "    }\n",
    "    return color_map.get(subject_id, \"#GRAY\")\n",
    "\n",
    "# 노드를 subject_id별로 그룹화\n",
    "grouped_data = data.groupby(\"f_subject_id\")\n",
    "y_positions = {sid: idx * 100 for idx, sid in enumerate(data[\"f_subject_id\"].unique())}\n",
    "\n",
    "# 노드 추가 (위치 정보 포함)\n",
    "for chapter_id, chapter_name, subject_id in zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"f_subject_id\"]):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color=get_node_color(subject_id),\n",
    "        size=30,\n",
    "        group=str(subject_id)  # 그룹 설정\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"#aaaaff\",  # 연한 파란색으로 변경\n",
    "        width=1,\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -100,\n",
    "      \"centralGravity\": 0.01,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.08,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"groups\": {\n",
    "    \"2212\": {\"color\": \"#FF4444\"},\n",
    "    \"2213\": {\"color\": \"#4444FF\"},\n",
    "    \"2214\": {\"color\": \"#44FF44\"},\n",
    "    \"2215\": {\"color\": \"#FFFF44\"}\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# Save visualization\n",
    "output_file = \"final_chapter_graph3.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph with integrated flow and similarity saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert embedding으로 군집화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER2\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "군집화 결과 분석:\n",
      "\n",
      "클러스터 0 (11 개 노드):\n",
      "chapter_name  subject_id\n",
      "     두수의크기비교        2212\n",
      "       덧셈과뺄셈        2212\n",
      "    덧셈과뺄셈의관계        2214\n",
      "       의값구하기        2214\n",
      "       세수의계산        2214\n",
      "   받아올림이있는덧셈        2213\n",
      "   받아내림이있는뺄셈        2213\n",
      "       덧셈하기2        2213\n",
      "       덧셈하기3        2213\n",
      "       뺄셈하기2        2213\n",
      "       뺄셈하기3        2213\n",
      "\n",
      "클러스터 1 (31 개 노드):\n",
      "       chapter_name  subject_id\n",
      "          12345알아보기        2212\n",
      "           6789알아보기        2212\n",
      "1만큼더큰수와1만큼더작은수0알아보기        2212\n",
      "        덧셈과뺄셈이야기만들기        2212\n",
      "                 덧셈        2212\n",
      "                 뺄셈        2212\n",
      "             10알아보기        2212\n",
      "             십몇알아보기        2212\n",
      "               묶어세기        2214\n",
      "           곱셈식을활용하기        2214\n",
      "             몇십알아보기        2213\n",
      "         99까지의수알아보기        2213\n",
      "           한자리세수의계산        2213\n",
      "             두수를더하기        2213\n",
      "           10이되는더하기        2213\n",
      "             10에서빼기        2213\n",
      "          10을만들어더하기        2213\n",
      "          몇시30분알아보기        2213\n",
      "         몇시몇시30분의응용        2213\n",
      "              덧셈하기1        2213\n",
      "              덧셈하기4        2213\n",
      "              뺄셈하기1        2213\n",
      "              뺄셈하기4        2213\n",
      "        1000과몇천알아보기        2215\n",
      "         1단곱셈구구와0의곱        2215\n",
      "                곱셈표        2215\n",
      "          곱셈구구를활용하기        2215\n",
      "               길이의합        2215\n",
      "               길이의차        2215\n",
      "          하루의시간알아보기        2215\n",
      "             달력알아보기        2215\n",
      "\n",
      "클러스터 2 (10 개 노드):\n",
      "  chapter_name  subject_id\n",
      "   2345를모으고가르기        2212\n",
      "   6789를모으고가르기        2212\n",
      "   몇십과몇십몇을알아보기        2212\n",
      "      백과몇백알아보기        2214\n",
      "      세자리수알아보기        2214\n",
      "         짝수와홀수        2213\n",
      "        몇시알아보기        2213\n",
      "10을이용하여모으기와가르기        2213\n",
      "      네자리수알아보기        2215\n",
      "         시각과시간        2215\n",
      "\n",
      "클러스터 3 (32 개 노드):\n",
      "chapter_name  subject_id\n",
      "     9까지수의순서        2212\n",
      "  여러가지모양찾아보기        2212\n",
      "  여러가지모양알아보기        2212\n",
      "      길이비교하기        2212\n",
      "      무게비교하기        2212\n",
      "      넓이비교하기        2212\n",
      "  담을수있는양비교하기        2212\n",
      "      높이비교하기        2212\n",
      "       키비교하기        2212\n",
      "   50까지의수의순서        2212\n",
      "        뛰어세기        2214\n",
      "          도형        2214\n",
      "        쌓기나무        2214\n",
      " 여러가지단위길이로재기        2214\n",
      "      자로길이재기        2214\n",
      "길이를어림해보고재어보기        2214\n",
      "        분류하기        2214\n",
      "     곱셈식알아보기        2214\n",
      "    수의순서알아보기        2213\n",
      "   두수의크기비교하기        2213\n",
      "여러개의수의크기비교하기        2213\n",
      "   여러가지모양만들기        2213\n",
      "        규칙찾기        2213\n",
      "      규칙만들기1        2213\n",
      "      규칙만들기2        2213\n",
      "       규칙찾기수        2213\n",
      "2단3단5단6단곱셈구구        2215\n",
      "4단7단8단9단곱셈구구        2215\n",
      "      길이알아보기        2215\n",
      "      길이어림하기        2215\n",
      "      표로나타내기        2215\n",
      "    그래프로나타내기        2215\n",
      "\n",
      "정리된 군집화 결과를 clustered_results.csv에 저장했습니다.\n",
      "\n",
      "Graph with embedding-based clustering saved to final_chapter_graph_clustered.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# BERT 임베딩 생성\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "embeddings_np = bert_embeddings.cpu().numpy()\n",
    "\n",
    "# K-means 군집화\n",
    "n_clusters = 4  # subject_id 개수와 동일하게 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings_np)\n",
    "\n",
    "# t-SNE로 차원 축소 (전체 데이터에 대해 한 번만 실행)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=min(30, len(embeddings_np) - 1),  # perplexity 값 조정\n",
    "    random_state=42,\n",
    "    n_iter=1000\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(embeddings_np)\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성\n",
    "flow_edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        flow_edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "cosine_sim = cosine_similarity(embeddings_np)\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if cosine_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 군집별 색상 매핑\n",
    "cluster_colors = ['#FF4444', '#4444FF', '#44FF44', '#FFFF44']\n",
    "\n",
    "# 노드 추가 (임베딩 기반 위치 정보 포함)\n",
    "for idx, (chapter_id, chapter_name) in enumerate(zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"])):\n",
    "    # t-SNE 좌표를 기반으로 위치 설정\n",
    "    x, y = embeddings_2d[idx]\n",
    "    \n",
    "    # 스케일 조정 (더 넓은 공간에 분포하도록)\n",
    "    x = x * 10\n",
    "    y = y * 10\n",
    "    \n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color=cluster_colors[cluster_labels[idx]],\n",
    "        size=30,\n",
    "        x=float(x),\n",
    "        y=float(y),\n",
    "        group=str(cluster_labels[idx])\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"#aaaaff\",\n",
    "        width=1,\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"groups\": {\n",
    "    \"0\": {\"color\": \"#FF4444\"},\n",
    "    \"1\": {\"color\": \"#4444FF\"},\n",
    "    \"2\": {\"color\": \"#44FF44\"},\n",
    "    \"3\": {\"color\": \"#FFFF44\"}\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# 군집화 결과 데이터프레임 생성\n",
    "cluster_info = pd.DataFrame({\n",
    "    'chapter_id': data['f_mchapter_id'],\n",
    "    'chapter_name': data['f_mchapter_nm'],\n",
    "    'subject_id': data['f_subject_id'],\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# 중복 제거 (chapter_name 기준으로 고유값만 유지)\n",
    "cluster_info_unique = cluster_info.drop_duplicates(subset=['chapter_name'])\n",
    "\n",
    "# 군집별 데이터 요약 출력\n",
    "print(\"\\n군집화 결과 분석:\")\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = cluster_info_unique[cluster_info_unique['cluster'] == cluster]\n",
    "    print(f\"\\n클러스터 {cluster} ({len(cluster_data)} 개 노드):\")\n",
    "    print(cluster_data[['chapter_name', 'subject_id']].to_string(index=False))\n",
    "\n",
    "# 저장할 경우 (옵션)\n",
    "output_file = \"clustered_results.csv\"\n",
    "cluster_info_unique.to_csv(output_file, index=False)\n",
    "print(f\"\\n정리된 군집화 결과를 {output_file}에 저장했습니다.\")\n",
    "\n",
    "\n",
    "# Save visualization\n",
    "output_file_html = \"final_chapter_graph_clustered.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with embedding-based clustering saved to {output_file_html}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원래 유사도 검사 + k-means 고정 & n_init 수 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER2\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "군집화 결과 분석:\n",
      "\n",
      "클러스터 0 (61 개 노드):\n",
      "       chapter_name  subject_id\n",
      "          12345알아보기        2212\n",
      "           6789알아보기        2212\n",
      "            9까지수의순서        2212\n",
      "1만큼더큰수와1만큼더작은수0알아보기        2212\n",
      "         여러가지모양찾아보기        2212\n",
      "         여러가지모양알아보기        2212\n",
      "        덧셈과뺄셈이야기만들기        2212\n",
      "                 덧셈        2212\n",
      "                 뺄셈        2212\n",
      "             길이비교하기        2212\n",
      "             무게비교하기        2212\n",
      "             넓이비교하기        2212\n",
      "         담을수있는양비교하기        2212\n",
      "             높이비교하기        2212\n",
      "              키비교하기        2212\n",
      "             10알아보기        2212\n",
      "             십몇알아보기        2212\n",
      "          50까지의수의순서        2212\n",
      "               뛰어세기        2214\n",
      "                 도형        2214\n",
      "               쌓기나무        2214\n",
      "        여러가지단위길이로재기        2214\n",
      "             자로길이재기        2214\n",
      "       길이를어림해보고재어보기        2214\n",
      "               분류하기        2214\n",
      "               묶어세기        2214\n",
      "            곱셈식알아보기        2214\n",
      "           곱셈식을활용하기        2214\n",
      "             몇십알아보기        2213\n",
      "         99까지의수알아보기        2213\n",
      "           수의순서알아보기        2213\n",
      "          두수의크기비교하기        2213\n",
      "       여러개의수의크기비교하기        2213\n",
      "           한자리세수의계산        2213\n",
      "             두수를더하기        2213\n",
      "           10이되는더하기        2213\n",
      "             10에서빼기        2213\n",
      "          10을만들어더하기        2213\n",
      "          여러가지모양만들기        2213\n",
      "          몇시30분알아보기        2213\n",
      "         몇시몇시30분의응용        2213\n",
      "               규칙찾기        2213\n",
      "             규칙만들기1        2213\n",
      "             규칙만들기2        2213\n",
      "              규칙찾기수        2213\n",
      "              덧셈하기1        2213\n",
      "              덧셈하기4        2213\n",
      "              뺄셈하기1        2213\n",
      "              뺄셈하기4        2213\n",
      "        1000과몇천알아보기        2215\n",
      "       2단3단5단6단곱셈구구        2215\n",
      "       4단7단8단9단곱셈구구        2215\n",
      "         1단곱셈구구와0의곱        2215\n",
      "                곱셈표        2215\n",
      "          곱셈구구를활용하기        2215\n",
      "             길이알아보기        2215\n",
      "               길이의합        2215\n",
      "               길이의차        2215\n",
      "             길이어림하기        2215\n",
      "          하루의시간알아보기        2215\n",
      "             달력알아보기        2215\n",
      "\n",
      "클러스터 1 (6 개 노드):\n",
      "chapter_name  subject_id\n",
      "     두수의크기비교        2212\n",
      "       세수의계산        2214\n",
      "   받아올림이있는덧셈        2213\n",
      "   받아내림이있는뺄셈        2213\n",
      "       덧셈하기2        2213\n",
      "       뺄셈하기2        2213\n",
      "\n",
      "클러스터 2 (12 개 노드):\n",
      "  chapter_name  subject_id\n",
      "   2345를모으고가르기        2212\n",
      "   6789를모으고가르기        2212\n",
      "   몇십과몇십몇을알아보기        2212\n",
      "      백과몇백알아보기        2214\n",
      "      세자리수알아보기        2214\n",
      "         짝수와홀수        2213\n",
      "        몇시알아보기        2213\n",
      "10을이용하여모으기와가르기        2213\n",
      "      네자리수알아보기        2215\n",
      "         시각과시간        2215\n",
      "        표로나타내기        2215\n",
      "      그래프로나타내기        2215\n",
      "\n",
      "클러스터 3 (5 개 노드):\n",
      "chapter_name  subject_id\n",
      "       덧셈과뺄셈        2212\n",
      "    덧셈과뺄셈의관계        2214\n",
      "       의값구하기        2214\n",
      "       덧셈하기3        2213\n",
      "       뺄셈하기3        2213\n",
      "\n",
      "정리된 군집화 결과를 clustered_results.csv에 저장했습니다.\n",
      "\n",
      "Graph with embedding-based clustering saved to final_chapter_graph_clustered2.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성\n",
    "flow_edges = []\n",
    "previous_id = None\n",
    "for _, row in data.iterrows():\n",
    "    current_id = row[\"f_mchapter_id\"]\n",
    "    if previous_id and previous_id != current_id:\n",
    "        flow_edges.append((previous_id, current_id))\n",
    "    previous_id = current_id\n",
    "\n",
    "# 유사도 기반 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 군집별 색상 매핑\n",
    "cluster_colors = ['#FF4444', '#4444FF', '#44FF44', '#FFFF44']\n",
    "\n",
    "# K-means 군집화\n",
    "n_clusters = 4  # subject_id 개수와 동일하게 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(bert_embeddings.cpu().numpy())\n",
    "\n",
    "# t-SNE로 차원 축소 (전체 데이터에 대해 한 번만 실행)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=min(30, len(bert_embeddings) - 1),  # perplexity 값 조정\n",
    "    random_state=42,\n",
    "    n_iter=1000\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(bert_embeddings.cpu().numpy())\n",
    "\n",
    "# 노드 추가 (임베딩 기반 위치 정보 포함)\n",
    "for idx, (chapter_id, chapter_name) in enumerate(zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"])):\n",
    "    # t-SNE 좌표를 기반으로 위치 설정\n",
    "    x, y = embeddings_2d[idx]\n",
    "    \n",
    "    # 스케일 조정 (더 넓은 공간에 분포하도록)\n",
    "    x = x * 10\n",
    "    y = y * 10\n",
    "    \n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color=cluster_colors[cluster_labels[idx]],\n",
    "        size=30,\n",
    "        x=float(x),\n",
    "        y=float(y),\n",
    "        group=str(cluster_labels[idx])\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"#aaaaff\",\n",
    "        width=1,\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"groups\": {\n",
    "    \"0\": {\"color\": \"#FF4444\"},\n",
    "    \"1\": {\"color\": \"#4444FF\"},\n",
    "    \"2\": {\"color\": \"#44FF44\"},\n",
    "    \"3\": {\"color\": \"#FFFF44\"}\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# 군집화 결과 데이터프레임 생성\n",
    "cluster_info = pd.DataFrame({\n",
    "    'chapter_id': data['f_mchapter_id'],\n",
    "    'chapter_name': data['f_mchapter_nm'],\n",
    "    'subject_id': data['f_subject_id'],\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# 중복 제거 (chapter_name 기준으로 고유값만 유지)\n",
    "cluster_info_unique = cluster_info.drop_duplicates(subset=['chapter_name'])\n",
    "\n",
    "# 군집별 데이터 요약 출력\n",
    "print(\"\\n군집화 결과 분석:\")\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = cluster_info_unique[cluster_info_unique['cluster'] == cluster]\n",
    "    print(f\"\\n클러스터 {cluster} ({len(cluster_data)} 개 노드):\")\n",
    "    print(cluster_data[['chapter_name', 'subject_id']].to_string(index=False))\n",
    "\n",
    "# 저장할 경우 (옵션)\n",
    "output_file = \"clustered_results.csv\"\n",
    "cluster_info_unique.to_csv(output_file, index=False)\n",
    "print(f\"\\n정리된 군집화 결과를 {output_file}에 저장했습니다.\")\n",
    "\n",
    "# Save visualization\n",
    "output_file_html = \"final_chapter_graph_clustered2.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with embedding-based clustering saved to {output_file_html}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 화살표 방향 : 시간의 흐름, 즉 학년 학기 순을 거스르지 않아야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER2\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "군집화 결과 분석:\n",
      "\n",
      "클러스터 0 (61 개 노드):\n",
      "       chapter_name  subject_id\n",
      "          12345알아보기        2212\n",
      "           6789알아보기        2212\n",
      "            9까지수의순서        2212\n",
      "1만큼더큰수와1만큼더작은수0알아보기        2212\n",
      "         여러가지모양찾아보기        2212\n",
      "         여러가지모양알아보기        2212\n",
      "        덧셈과뺄셈이야기만들기        2212\n",
      "                 덧셈        2212\n",
      "                 뺄셈        2212\n",
      "             길이비교하기        2212\n",
      "             무게비교하기        2212\n",
      "             넓이비교하기        2212\n",
      "         담을수있는양비교하기        2212\n",
      "             높이비교하기        2212\n",
      "              키비교하기        2212\n",
      "             10알아보기        2212\n",
      "             십몇알아보기        2212\n",
      "          50까지의수의순서        2212\n",
      "               뛰어세기        2214\n",
      "                 도형        2214\n",
      "               쌓기나무        2214\n",
      "        여러가지단위길이로재기        2214\n",
      "             자로길이재기        2214\n",
      "       길이를어림해보고재어보기        2214\n",
      "               분류하기        2214\n",
      "               묶어세기        2214\n",
      "            곱셈식알아보기        2214\n",
      "           곱셈식을활용하기        2214\n",
      "             몇십알아보기        2213\n",
      "         99까지의수알아보기        2213\n",
      "           수의순서알아보기        2213\n",
      "          두수의크기비교하기        2213\n",
      "       여러개의수의크기비교하기        2213\n",
      "           한자리세수의계산        2213\n",
      "             두수를더하기        2213\n",
      "           10이되는더하기        2213\n",
      "             10에서빼기        2213\n",
      "          10을만들어더하기        2213\n",
      "          여러가지모양만들기        2213\n",
      "          몇시30분알아보기        2213\n",
      "         몇시몇시30분의응용        2213\n",
      "               규칙찾기        2213\n",
      "             규칙만들기1        2213\n",
      "             규칙만들기2        2213\n",
      "              규칙찾기수        2213\n",
      "              덧셈하기1        2213\n",
      "              덧셈하기4        2213\n",
      "              뺄셈하기1        2213\n",
      "              뺄셈하기4        2213\n",
      "        1000과몇천알아보기        2215\n",
      "       2단3단5단6단곱셈구구        2215\n",
      "       4단7단8단9단곱셈구구        2215\n",
      "         1단곱셈구구와0의곱        2215\n",
      "                곱셈표        2215\n",
      "          곱셈구구를활용하기        2215\n",
      "             길이알아보기        2215\n",
      "               길이의합        2215\n",
      "               길이의차        2215\n",
      "             길이어림하기        2215\n",
      "          하루의시간알아보기        2215\n",
      "             달력알아보기        2215\n",
      "\n",
      "클러스터 1 (6 개 노드):\n",
      "chapter_name  subject_id\n",
      "     두수의크기비교        2212\n",
      "       세수의계산        2214\n",
      "   받아올림이있는덧셈        2213\n",
      "   받아내림이있는뺄셈        2213\n",
      "       덧셈하기2        2213\n",
      "       뺄셈하기2        2213\n",
      "\n",
      "클러스터 2 (12 개 노드):\n",
      "  chapter_name  subject_id\n",
      "   2345를모으고가르기        2212\n",
      "   6789를모으고가르기        2212\n",
      "   몇십과몇십몇을알아보기        2212\n",
      "      백과몇백알아보기        2214\n",
      "      세자리수알아보기        2214\n",
      "         짝수와홀수        2213\n",
      "        몇시알아보기        2213\n",
      "10을이용하여모으기와가르기        2213\n",
      "      네자리수알아보기        2215\n",
      "         시각과시간        2215\n",
      "        표로나타내기        2215\n",
      "      그래프로나타내기        2215\n",
      "\n",
      "클러스터 3 (5 개 노드):\n",
      "chapter_name  subject_id\n",
      "       덧셈과뺄셈        2212\n",
      "    덧셈과뺄셈의관계        2214\n",
      "       의값구하기        2214\n",
      "       덧셈하기3        2213\n",
      "       뺄셈하기3        2213\n",
      "\n",
      "정리된 군집화 결과를 clustered_results3.csv에 저장했습니다.\n",
      "\n",
      "Graph with embedding-based clustering saved to final_chapter_graph_clustered3.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준 흐름 기반 관계 생성 (f_subject_id를 고려한 방향 설정)\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    \n",
    "    # f_subject_id 값 기준으로 방향 설정\n",
    "    if current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"]:\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 기반 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "threshold = 0.95\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 군집별 색상 매핑\n",
    "cluster_colors = ['#FF4444', '#4444FF', '#44FF44', '#FFFF44']\n",
    "\n",
    "# K-means 군집화\n",
    "n_clusters = 4  # subject_id 개수와 동일하게 설정\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n",
    "cluster_labels = kmeans.fit_predict(bert_embeddings.cpu().numpy())\n",
    "\n",
    "# t-SNE로 차원 축소 (전체 데이터에 대해 한 번만 실행)\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=min(30, len(bert_embeddings) - 1),  # perplexity 값 조정\n",
    "    random_state=42,\n",
    "    n_iter=1000\n",
    ")\n",
    "embeddings_2d = tsne.fit_transform(bert_embeddings.cpu().numpy())\n",
    "\n",
    "# 노드 추가 (임베딩 기반 위치 정보 포함)\n",
    "for idx, (chapter_id, chapter_name) in enumerate(zip(data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"])):\n",
    "    # t-SNE 좌표를 기반으로 위치 설정\n",
    "    x, y = embeddings_2d[idx]\n",
    "    \n",
    "    # 스케일 조정 (더 넓은 공간에 분포하도록)\n",
    "    x = x * 10\n",
    "    y = y * 10\n",
    "    \n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color=cluster_colors[cluster_labels[idx]],\n",
    "        size=30,\n",
    "        x=float(x),\n",
    "        y=float(y),\n",
    "        group=str(cluster_labels[idx])\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"#aaaaff\",\n",
    "        width=1,\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"groups\": {\n",
    "    \"0\": {\"color\": \"#FF4444\"},\n",
    "    \"1\": {\"color\": \"#4444FF\"},\n",
    "    \"2\": {\"color\": \"#44FF44\"},\n",
    "    \"3\": {\"color\": \"#FFFF44\"}\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# 군집화 결과 데이터프레임 생성\n",
    "cluster_info = pd.DataFrame({\n",
    "    'chapter_id': data['f_mchapter_id'],\n",
    "    'chapter_name': data['f_mchapter_nm'],\n",
    "    'subject_id': data['f_subject_id'],\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# 중복 제거 (chapter_name 기준으로 고유값만 유지)\n",
    "cluster_info_unique = cluster_info.drop_duplicates(subset=['chapter_name'])\n",
    "\n",
    "# 군집별 데이터 요약 출력\n",
    "print(\"\\n군집화 결과 분석:\")\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_data = cluster_info_unique[cluster_info_unique['cluster'] == cluster]\n",
    "    print(f\"\\n클러스터 {cluster} ({len(cluster_data)} 개 노드):\")\n",
    "    print(cluster_data[['chapter_name', 'subject_id']].to_string(index=False))\n",
    "\n",
    "# 저장할 경우 (옵션)\n",
    "output_file = \"clustered_results3.csv\"\n",
    "cluster_info_unique.to_csv(output_file, index=False)\n",
    "print(f\"\\n정리된 군집화 결과를 {output_file}에 저장했습니다.\")\n",
    "\n",
    "# Save visualization\n",
    "output_file_html = \"final_chapter_graph_clustered3.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with embedding-based clustering saved to {output_file_html}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 화살표 방향(역순흐름X) & 2022년 개정 수학 교육부 성취기준 4개 영역(수와연산,변화와관계,도형과측정,자료와가능성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph with embedding-based clustering saved to final_chapter_graph_clustered4.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 군집화 및 학년 학기 색상 매핑\n",
    "area_colors = {\n",
    "    \"수와 연산\": \"#FFCCCC\",\n",
    "    \"변화와 관계\": \"#CCCCFF\",\n",
    "    \"도형과 측정\": \"#CCFFCC\",\n",
    "    \"자료와 가능성\": \"#FFFFCC\",\n",
    "    \"기타\": \"#CCCCCC\"\n",
    "}\n",
    "subject_colors = {\n",
    "    2212: \"#FF0000\",  # 1학년 1학기\n",
    "    2213: \"#0000FF\",  # 1학년 2학기\n",
    "    2214: \"#00FF00\",  # 2학년 1학기\n",
    "    2215: \"#FFFF00\"   # 2학년 2학기\n",
    "}\n",
    "\n",
    "def get_area_by_code(code):\n",
    "    if code.startswith(\"2수01\"):\n",
    "        return \"수와 연산\"\n",
    "    elif code.startswith(\"2수02\"):\n",
    "        return \"변화와 관계\"\n",
    "    elif code.startswith(\"2수03\"):\n",
    "        return \"도형과 측정\"\n",
    "    elif code.startswith(\"2수04\"):\n",
    "        return \"자료와 가능성\"\n",
    "    else:\n",
    "        return \"기타\"\n",
    "\n",
    "data[\"area\"] = data[\"성취기준코드\"].apply(get_area_by_code)\n",
    "data[\"color\"] = data[\"area\"].map(area_colors)\n",
    "data[\"border_color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 흐름 기반 엣지 생성\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    if (current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"] or\n",
    "        (current_row[\"f_subject_id\"] == next_row[\"f_subject_id\"] and\n",
    "         current_row[\"성취기준코드\"] < next_row[\"성취기준코드\"])):\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "threshold = 0.85\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "nodes_json = []\n",
    "for idx, (chapter_id, chapter_name, color, border_color) in enumerate(zip(\n",
    "    data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"color\"], data[\"border_color\"]\n",
    ")):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color={\n",
    "            \"background\": color,\n",
    "            \"border\": border_color\n",
    "        },\n",
    "        size=30,\n",
    "        borderWidth=2\n",
    "    )\n",
    "    nodes_json.append({\n",
    "        \"id\": chapter_id,\n",
    "        \"name\": chapter_name,\n",
    "        \"area\": data.loc[idx, \"area\"],\n",
    "        \"subject_id\": data.loc[idx, \"f_subject_id\"],\n",
    "        \"color\": color,\n",
    "        \"border_color\": border_color\n",
    "    })\n",
    "\n",
    "# 엣지 추가\n",
    "edges_json = []\n",
    "for source, target in final_edges:\n",
    "    if str(source) in graph.nodes and str(target) in graph.nodes and source != target:\n",
    "        graph.add_edge(\n",
    "            str(source), str(target),\n",
    "            color=\"#aaaaff\",\n",
    "            width=1,\n",
    "            title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "        )\n",
    "        edges_json.append({\n",
    "            \"source\": source,\n",
    "            \"target\": target\n",
    "        })\n",
    "\n",
    "# PyVis 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# 그래프 저장\n",
    "output_file_html = \"final_chapter_graph_clustered4.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with embedding-based clustering saved to {output_file_html}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to graph_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^\\uac00-\\ud7a3a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 흐름 기반 엣지 생성\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    if (current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"] or\n",
    "        (current_row[\"f_subject_id\"] == next_row[\"f_subject_id\"] and\n",
    "         current_row[\"성취기준코드\"] < next_row[\"성취기준코드\"])):\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "threshold = 0.85\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_id = str(data.iloc[i][\"f_mchapter_id\"])\n",
    "            target_id = str(data.iloc[j][\"f_mchapter_id\"])\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# Combine edges\n",
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# 노드 JSON 생성\n",
    "nodes_json = []\n",
    "for idx, (chapter_id, chapter_name, subject_id, 성취기준코드) in enumerate(zip(\n",
    "    data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"f_subject_id\"], data[\"성취기준코드\"]\n",
    ")):\n",
    "    nodes_json.append({\n",
    "        \"id\": str(chapter_id),\n",
    "        \"name\": chapter_name,\n",
    "        \"subject_id\": int(subject_id),\n",
    "        \"code\": 성취기준코드\n",
    "    })\n",
    "\n",
    "# 엣지 JSON 생성\n",
    "edges_json = []\n",
    "for source, target in final_edges:\n",
    "    edges_json.append({\n",
    "        \"source\": str(source),\n",
    "        \"target\": str(target)\n",
    "    })\n",
    "\n",
    "# 결과 JSON 저장\n",
    "output_json = {\n",
    "    \"nodes\": nodes_json,\n",
    "    \"edges\": edges_json\n",
    "}\n",
    "output_file = \"graph_data.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 매트릭스 샘플:\n",
      " [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"유사도 매트릭스 샘플:\\n\", np.round(final_sim[:5, :5], 2))  # 상위 5개 행/열만 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85  # 기준\n",
    "similar_pairs = np.argwhere(final_sim >= threshold)\n",
    "for i, j in similar_pairs:\n",
    "    if i != j:\n",
    "        print(f\"Node {i} ↔ Node {j} : {final_sim[i, j]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "수와 연산      503\n",
      "도형과 측정     237\n",
      "자료와 가능성     32\n",
      "변화와 관계      32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"area\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_subject_id\n",
      "2214    228\n",
      "2213    197\n",
      "2212    192\n",
      "2215    187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data[\"f_subject_id\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Edges Sample:\n",
      " [(14201780, 14201781), (14201781, 14201782), (14201782, 14201783), (14201783, 14201784), (14201784, 14201785)]\n",
      "Similarity Edges Sample:\n",
      " [(14201779, 14201780), (14201780, 14201798), (14201781, 14201801), (14201782, 14201788), (14201783, 14201790)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Flow Edges Sample:\\n\", flow_edges[:5])  # 흐름 기반 엣지 상위 5개\n",
    "print(\"Similarity Edges Sample:\\n\", similarity_edges[:5])  # 유사도 기반 엣지 상위 5개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Edges: 136\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Edges: {len(final_edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow edges 개수: 68\n",
      "Similarity edges 개수: 77\n"
     ]
    }
   ],
   "source": [
    "print(f\"Flow edges 개수: {len(flow_edges)}\")\n",
    "print(f\"Similarity edges 개수: {len(similarity_edges)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 엣지 개수: 136\n"
     ]
    }
   ],
   "source": [
    "final_edges = list(set(flow_edges + similarity_edges))\n",
    "print(f\"최종 엣지 개수: {len(final_edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 중단원 노드 수: 94\n"
     ]
    }
   ],
   "source": [
    "unique_chapters = data[\"f_mchapter_id\"].nunique()\n",
    "print(f\"고유 중단원 노드 수: {unique_chapters}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 데이터 수: 800\n"
     ]
    }
   ],
   "source": [
    "duplicated_rows = data.duplicated(subset=[\"f_mchapter_id\"], keep=False).sum()\n",
    "print(f\"중복된 데이터 수: {duplicated_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 행 수: 804\n",
      "고유 중단원 노드 수: 94\n"
     ]
    }
   ],
   "source": [
    "print(f\"전체 데이터 행 수: {len(data)}\")\n",
    "print(f\"고유 중단원 노드 수: {unique_chapters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow edges 개수: 68\n",
      "Similarity edges 개수: 77\n",
      "최종 엣지 개수: 136\n"
     ]
    }
   ],
   "source": [
    "print(f\"Flow edges 개수: {len(flow_edges)}\")\n",
    "print(f\"Similarity edges 개수: {len(similarity_edges)}\")\n",
    "print(f\"최종 엣지 개수: {len(final_edges)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도 조건 미달 노드 수: 0\n"
     ]
    }
   ],
   "source": [
    "isolated_nodes = [\n",
    "    i for i in range(len(data))\n",
    "    if all(final_sim[i, j] < threshold for j in range(len(data)) if i != j)\n",
    "]\n",
    "print(f\"유사도 조건 미달 노드 수: {len(isolated_nodes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엣지 연결 93개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph with sequential connections saved to linear_graph.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^\\uac00-\\ud7a3a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 군집화 및 학년 학기 색상 매핑\n",
    "area_colors = {\n",
    "    \"수와 연산\": \"#FFCCCC\",\n",
    "    \"변화와 관계\": \"#CCCCFF\",\n",
    "    \"도형과 측정\": \"#CCFFCC\",\n",
    "    \"자료와 가능성\": \"#FFFFCC\",\n",
    "    \"기타\": \"#CCCCCC\"\n",
    "}\n",
    "subject_colors = {\n",
    "    2212: \"#FF0000\",  # 1학년 1학기\n",
    "    2213: \"#0000FF\",  # 1학년 2학기\n",
    "    2214: \"#00FF00\",  # 2학년 1학기\n",
    "    2215: \"#FFFF00\"   # 2학년 2학기\n",
    "}\n",
    "\n",
    "def get_area_by_code(code):\n",
    "    if code.startswith(\"2수01\"):\n",
    "        return \"수와 연산\"\n",
    "    elif code.startswith(\"2수02\"):\n",
    "        return \"변화와 관계\"\n",
    "    elif code.startswith(\"2수03\"):\n",
    "        return \"도형과 측정\"\n",
    "    elif code.startswith(\"2수04\"):\n",
    "        return \"자료와 가능성\"\n",
    "    else:\n",
    "        return \"기타\"\n",
    "\n",
    "data[\"area\"] = data[\"성취기준코드\"].apply(get_area_by_code)\n",
    "data[\"color\"] = data[\"area\"].map(area_colors)\n",
    "data[\"border_color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 노드를 시간 흐름 순서로 정렬 후 선형 연결\n",
    "sorted_nodes = data.sort_values(by=[\"f_subject_id\", \"성취기준코드\"])[\"f_mchapter_id\"].unique()\n",
    "sequential_edges = [(sorted_nodes[i], sorted_nodes[i + 1]) for i in range(len(sorted_nodes) - 1)]\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "for idx, (chapter_id, chapter_name, color, border_color) in enumerate(zip(\n",
    "    data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"color\"], data[\"border_color\"]\n",
    ")):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color={\n",
    "            \"background\": color,\n",
    "            \"border\": border_color\n",
    "        },\n",
    "        size=30,\n",
    "        borderWidth=2\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in sequential_edges:\n",
    "    graph.add_edge(\n",
    "        str(source), str(target),\n",
    "        color=\"#aaaaff\",\n",
    "        width=1,\n",
    "        title=\"뭐지지\"\n",
    "    )\n",
    "\n",
    "# PyVis 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''\n",
    "{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''')\n",
    "\n",
    "# 그래프 저장\n",
    "output_file_html = \"linear_graph.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with sequential connections saved to {output_file_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential edges 개수: 93\n"
     ]
    }
   ],
   "source": [
    "# 순차적 연결(시간 흐름 기반) 엣지 개수 확인\n",
    "print(f\"Sequential edges 개수: {len(sequential_edges)}\")  # 시간 흐름 기반 선형 연결의 개수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nodes = data.sort_values(by=[\"f_subject_id\", \"성취기준코드\"])[\"f_mchapter_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source 노드 정보:    f_subject_id   성취기준코드\n",
      "0          2212  2수01-01\n",
      "1          2212  2수01-01\n",
      "2          2212  2수01-01\n",
      "3          2212  2수01-01\n",
      "4          2212  2수01-01\n",
      "5          2212  2수01-01\n",
      "Target 노드 정보:     f_subject_id   성취기준코드\n",
      "6           2212  2수01-01\n",
      "7           2212  2수01-01\n",
      "8           2212  2수01-01\n",
      "9           2212  2수01-01\n",
      "10          2212  2수01-01\n"
     ]
    }
   ],
   "source": [
    "source_row = data[data[\"f_mchapter_id\"] == 14201779]\n",
    "target_row = data[data[\"f_mchapter_id\"] == 14201780]\n",
    "\n",
    "print(\"Source 노드 정보:\", source_row[[\"f_subject_id\", \"성취기준코드\"]])\n",
    "print(\"Target 노드 정보:\", target_row[[\"f_subject_id\", \"성취기준코드\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=[\"f_subject_id\", \"성취기준코드\"])\n",
    "sorted_nodes = data[\"f_mchapter_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   f_mchapter_id  f_subject_id   성취기준코드\n",
      "0       14201779          2212  2수01-01\n",
      "1       14201779          2212  2수01-01\n",
      "2       14201779          2212  2수01-01\n",
      "3       14201779          2212  2수01-01\n",
      "4       14201779          2212  2수01-01\n",
      "5       14201779          2212  2수01-01\n",
      "6       14201780          2212  2수01-01\n",
      "7       14201780          2212  2수01-01\n",
      "8       14201780          2212  2수01-01\n",
      "9       14201780          2212  2수01-01\n"
     ]
    }
   ],
   "source": [
    "print(data[[\"f_mchapter_id\", \"f_subject_id\", \"성취기준코드\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코드 순서 경고(무시 가능): 14201779 → 14201780\n",
      "코드 순서 경고(무시 가능): 14201780 → 14201798\n",
      "코드 순서 경고(무시 가능): 14201798 → 14201799\n",
      "코드 순서 경고(무시 가능): 14201781 → 14201801\n",
      "코드 순서 경고(무시 가능): 14201801 → 14201802\n",
      "코드 순서 경고(무시 가능): 14201786 → 14201787\n",
      "코드 순서 경고(무시 가능): 14201782 → 14201788\n",
      "코드 순서 경고(무시 가능): 14201788 → 14201789\n",
      "코드 순서 경고(무시 가능): 14201789 → 14201790\n",
      "코드 순서 경고(무시 가능): 14201792 → 14201793\n",
      "코드 순서 경고(무시 가능): 14201793 → 14201794\n",
      "코드 순서 경고(무시 가능): 14201794 → 14201795\n",
      "코드 순서 경고(무시 가능): 14201795 → 14201796\n",
      "코드 순서 경고(무시 가능): 14201796 → 14201797\n",
      "코드 순서 경고(무시 가능): 14201857 → 14201858\n",
      "코드 순서 경고(무시 가능): 14201859 → 14201860\n",
      "코드 순서 경고(무시 가능): 14201860 → 14201861\n",
      "코드 순서 경고(무시 가능): 14201862 → 14201874\n",
      "코드 순서 경고(무시 가능): 14201863 → 14201864\n",
      "코드 순서 경고(무시 가능): 14201864 → 14201865\n",
      "코드 순서 경고(무시 가능): 14201865 → 14201866\n",
      "코드 순서 경고(무시 가능): 14201866 → 14201867\n",
      "코드 순서 경고(무시 가능): 14201867 → 14201881\n",
      "코드 순서 경고(무시 가능): 14201881 → 14201884\n",
      "코드 순서 경고(무시 가능): 14201884 → 14201885\n",
      "코드 순서 경고(무시 가능): 14201885 → 14201888\n",
      "코드 순서 경고(무시 가능): 14201875 → 14201876\n",
      "코드 순서 경고(무시 가능): 14201876 → 14201882\n",
      "코드 순서 경고(무시 가능): 14201882 → 14201886\n",
      "코드 순서 경고(무시 가능): 14201883 → 14201887\n",
      "코드 순서 경고(무시 가능): 14201877 → 14201878\n",
      "코드 순서 경고(무시 가능): 14201878 → 14201880\n",
      "코드 순서 경고(무시 가능): 14201868 → 14201869\n",
      "코드 순서 경고(무시 가능): 14201803 → 14201804\n",
      "코드 순서 경고(무시 가능): 14201805 → 14201806\n",
      "코드 순서 경고(무시 가능): 14201819 → 14201820\n",
      "코드 순서 경고(무시 가능): 14201820 → 14201821\n",
      "코드 순서 경고(무시 가능): 14201815 → 14201816\n",
      "코드 순서 경고(무시 가능): 14201890 → 14201892\n",
      "코드 순서 경고(무시 가능): 14201893 → 14201894\n",
      "코드 순서 경고(무시 가능): 14201894 → 14201895\n",
      "코드 순서 경고(무시 가능): 14201895 → 14201896\n",
      "코드 순서 경고(무시 가능): 14201896 → 14201897\n",
      "코드 순서 경고(무시 가능): 14201903 → 14201904\n",
      "코드 순서 경고(무시 가능): 14201899 → 14201900\n"
     ]
    }
   ],
   "source": [
    "for source, target in sequential_edges:\n",
    "    source_row = data[data[\"f_mchapter_id\"] == source]\n",
    "    target_row = data[data[\"f_mchapter_id\"] == target]\n",
    "\n",
    "    # 학기 순서만 비교\n",
    "    assert source_row[\"f_subject_id\"].values[0] <= target_row[\"f_subject_id\"].values[0], \\\n",
    "        f\"학기 순서 오류: {source} → {target}\"\n",
    "    \n",
    "    # 동일 학기 내에서 성취기준코드 비교\n",
    "    if source_row[\"f_subject_id\"].values[0] == target_row[\"f_subject_id\"].values[0]:\n",
    "        if source_row[\"성취기준코드\"].values[0] >= target_row[\"성취기준코드\"].values[0]:\n",
    "            print(f\"코드 순서 경고(무시 가능): {source} → {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노드 개수: 94\n",
      "엣지 개수: 93\n"
     ]
    }
   ],
   "source": [
    "print(f\"노드 개수: {len(data['f_mchapter_id'].unique())}\")\n",
    "print(f\"엣지 개수: {len(sequential_edges)}\")\n",
    "assert len(sequential_edges) == len(data['f_mchapter_id'].unique()) - 1, \"엣지 개수가 예상과 다릅니다!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph with optimal connections saved to optimal_graph2.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^\\uac00-\\ud7a3a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 군집화 및 학년 학기 색상 매핑\n",
    "area_colors = {\n",
    "    \"수와 연산\": \"#FFCCCC\",\n",
    "    \"변화와 관계\": \"#CCCCFF\",\n",
    "    \"도형과 측정\": \"#CCFFCC\",\n",
    "    \"자료와 가능성\": \"#FFFFCC\",\n",
    "    \"기타\": \"#CCCCCC\"\n",
    "}\n",
    "subject_colors = {\n",
    "    2212: \"#FF0000\",  # 1학년 1학기\n",
    "    2213: \"#0000FF\",  # 1학년 2학기\n",
    "    2214: \"#00FF00\",  # 2학년 1학기\n",
    "    2215: \"#FFFF00\"   # 2학년 2학기\n",
    "}\n",
    "\n",
    "def get_area_by_code(code):\n",
    "    if code.startswith(\"2수01\"):\n",
    "        return \"수와 연산\"\n",
    "    elif code.startswith(\"2수02\"):\n",
    "        return \"변화와 관계\"\n",
    "    elif code.startswith(\"2수03\"):\n",
    "        return \"도형과 측정\"\n",
    "    elif code.startswith(\"2수04\"):\n",
    "        return \"자료와 가능성\"\n",
    "    else:\n",
    "        return \"기타\"\n",
    "\n",
    "data[\"area\"] = data[\"성취기준코드\"].apply(get_area_by_code)\n",
    "data[\"color\"] = data[\"area\"].map(area_colors)\n",
    "data[\"border_color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 노드를 시간 흐름 순서로 정렬 후 선형 연결\n",
    "sorted_nodes = data.sort_values(by=[\"f_subject_id\", \"성취기준코드\"])[\"f_mchapter_id\"].unique()\n",
    "sequential_edges = [(sorted_nodes[i], sorted_nodes[i + 1]) for i in range(len(sorted_nodes) - 1)]\n",
    "\n",
    "# 유사도 기반 연결 생성\n",
    "threshold = 0.85  # 유사도 임계값\n",
    "similarity_edges = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] >= threshold:  # 자기 자신 제외\n",
    "            similarity_edges.append((data.iloc[i][\"f_mchapter_id\"], data.iloc[j][\"f_mchapter_id\"], final_sim[i, j]))\n",
    "\n",
    "# 가장 강력한 연결 필터링\n",
    "strongest_edges = []\n",
    "node_connections = {}\n",
    "\n",
    "for source, target, sim_score in similarity_edges:\n",
    "    if source not in node_connections or node_connections[source][1] < sim_score:\n",
    "        node_connections[source] = (target, sim_score)\n",
    "\n",
    "for source, (target, sim_score) in node_connections.items():\n",
    "    if source != target:  # 자기 자신 제외\n",
    "        strongest_edges.append((source, target))\n",
    "\n",
    "# 시간 흐름과 유사도 기반 연결 통합\n",
    "final_edges = list(set(sequential_edges + strongest_edges))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "for idx, (chapter_id, chapter_name, color, border_color) in enumerate(zip(\n",
    "    data[\"f_mchapter_id\"], data[\"f_mchapter_nm\"], data[\"color\"], data[\"border_color\"]\n",
    ")):\n",
    "    graph.add_node(\n",
    "        str(chapter_id),\n",
    "        label=chapter_name,\n",
    "        color={\n",
    "            \"background\": color,\n",
    "            \"border\": border_color\n",
    "        },\n",
    "        size=30,\n",
    "        borderWidth=2\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    if source != target:  # 자기 자신을 가리키는 엣지 제거\n",
    "        graph.add_edge(\n",
    "            str(source), str(target),\n",
    "            color=\"#ffa500\",  # 점수 기반 최적 연결\n",
    "            width=1,\n",
    "            title=\"최적 연결\"\n",
    "        )\n",
    "\n",
    "# PyVis 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정\n",
    "net.set_options('''{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}''')\n",
    "\n",
    "# 그래프 저장\n",
    "output_file_html = \"optimal_graph2.html\"\n",
    "net.write_html(output_file_html)\n",
    "print(f\"\\nGraph with optimal connections saved to {output_file_html}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow edges: 68\n",
      "Similarity edges (First Code): 38470\n",
      "Final edges (First Code): 163\n",
      "Sequential edges: 93\n",
      "Similarity edges (Second Code): 72\n",
      "Final edges (Second Code): 163\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 코드\n",
    "print(f\"Flow edges: {len(flow_edges)}\")\n",
    "print(f\"Similarity edges (First Code): {len(similarity_edges)}\")\n",
    "print(f\"Final edges (First Code): {len(final_edges)}\")\n",
    "\n",
    "# 두 번째 코드\n",
    "print(f\"Sequential edges: {len(sequential_edges)}\")\n",
    "print(f\"Similarity edges (Second Code): {len(strongest_edges)}\")\n",
    "print(f\"Final edges (Second Code): {len(final_edges)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시간 제한 적용과 하지 않은 건 무슨 차이가 있나.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graphs saved: 'optimal_graph_no_time.html' (No Time Restriction), 'optimal_graph_with_time.html' (With Time Restriction)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^\\uac00-\\ud7a3a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 군집화 및 학년 학기 색상 매핑\n",
    "area_colors = {\n",
    "    \"수와 연산\": \"#FFCCCC\",\n",
    "    \"변화와 관계\": \"#CCCCFF\",\n",
    "    \"도형과 측정\": \"#CCFFCC\",\n",
    "    \"자료와 가능성\": \"#FFFFCC\",\n",
    "    \"기타\": \"#CCCCCC\"\n",
    "}\n",
    "subject_colors = {\n",
    "    2212: \"#FF0000\",  # 1학년 1학기\n",
    "    2213: \"#0000FF\",  # 1학년 2학기\n",
    "    2214: \"#00FF00\",  # 2학년 1학기\n",
    "    2215: \"#FFFF00\"   # 2학년 2학기\n",
    "}\n",
    "\n",
    "def get_area_by_code(code):\n",
    "    if code.startswith(\"2수01\"):\n",
    "        return \"수와 연산\"\n",
    "    elif code.startswith(\"2수02\"):\n",
    "        return \"변화와 관계\"\n",
    "    elif code.startswith(\"2수03\"):\n",
    "        return \"도형과 측정\"\n",
    "    elif code.startswith(\"2수04\"):\n",
    "        return \"자료와 가능성\"\n",
    "    else:\n",
    "        return \"기타\"\n",
    "\n",
    "data[\"area\"] = data[\"성취기준코드\"].apply(get_area_by_code)\n",
    "data[\"color\"] = data[\"area\"].map(area_colors)\n",
    "data[\"border_color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 흐름 기반 엣지 생성\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    if (current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"] or\n",
    "        (current_row[\"f_subject_id\"] == next_row[\"f_subject_id\"] and\n",
    "         current_row[\"성취기준코드\"] < next_row[\"성취기준코드\"])):\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 기반 연결 생성 (시간 제한 적용 또는 미적용)\n",
    "def create_similarity_edges(data, final_sim, threshold=0.85, time_restricted=False):\n",
    "    similarity_edges = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            if i != j and final_sim[i, j] >= threshold:\n",
    "                source = data.iloc[i]\n",
    "                target = data.iloc[j]\n",
    "                # 시간 제한 조건\n",
    "                if time_restricted:\n",
    "                    if source[\"f_subject_id\"] >= target[\"f_subject_id\"]:  # 선행 학습 제한\n",
    "                        continue\n",
    "                similarity_edges.append((source[\"f_mchapter_id\"], target[\"f_mchapter_id\"]))\n",
    "    return similarity_edges\n",
    "\n",
    "# 유사도 기반 연결 생성\n",
    "similarity_edges_no_time = create_similarity_edges(data, final_sim, time_restricted=False)\n",
    "similarity_edges_with_time = create_similarity_edges(data, final_sim, time_restricted=True)\n",
    "\n",
    "# 최적의 엣지 생성\n",
    "final_edges_no_time = list(set(flow_edges + similarity_edges_no_time))\n",
    "final_edges_with_time = list(set(flow_edges + similarity_edges_with_time))\n",
    "\n",
    "# 그래프 생성 (시간 제한 미적용)\n",
    "graph_no_time = nx.DiGraph()\n",
    "for source, target in final_edges_no_time:\n",
    "    if source != target:\n",
    "        graph_no_time.add_edge(\n",
    "            str(source), str(target),\n",
    "            color=\"#ffa500\",  # 점수 기반 최적 연결\n",
    "            width=1,\n",
    "            title=\"유사도 기반 연결 (시간 제한 없음)\"\n",
    "        )\n",
    "\n",
    "# 그래프 생성 (시간 제한 적용)\n",
    "graph_with_time = nx.DiGraph()\n",
    "for source, target in final_edges_with_time:\n",
    "    if source != target:\n",
    "        graph_with_time.add_edge(\n",
    "            str(source), str(target),\n",
    "            color=\"#aaaaff\",  # 점수 기반 최적 연결\n",
    "            width=1,\n",
    "            title=\"유사도 기반 연결 (시간 제한 적용)\"\n",
    "        )\n",
    "\n",
    "# PyVis 시각화\n",
    "net_no_time = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net_no_time.from_nx(graph_no_time)\n",
    "net_no_time.set_options('''{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}''')\n",
    "\n",
    "net_with_time = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net_with_time.from_nx(graph_with_time)\n",
    "net_with_time.set_options('''{\n",
    "  \"physics\": {\n",
    "    \"enabled\": true,\n",
    "    \"forceAtlas2Based\": {\n",
    "      \"gravitationalConstant\": -50,\n",
    "      \"centralGravity\": 0.005,\n",
    "      \"springLength\": 100,\n",
    "      \"springConstant\": 0.04,\n",
    "      \"damping\": 0.4,\n",
    "      \"avoidOverlap\": 1\n",
    "    },\n",
    "    \"solver\": \"forceAtlas2Based\",\n",
    "    \"stabilization\": {\n",
    "      \"enabled\": true,\n",
    "      \"iterations\": 2000,\n",
    "      \"updateInterval\": 25\n",
    "    }\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 25,\n",
    "    \"font\": {\n",
    "      \"size\": 14\n",
    "    }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\",\n",
    "      \"forceDirection\": \"none\"\n",
    "    },\n",
    "    \"arrows\": {\n",
    "      \"to\": {\n",
    "        \"enabled\": true,\n",
    "        \"scaleFactor\": 0.5\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}''')\n",
    "\n",
    "# 그래프 저장\n",
    "net_no_time.write_html(\"optimal_graph_no_time.html\")\n",
    "net_with_time.write_html(\"optimal_graph_with_time.html\")\n",
    "print(f\"\\nGraphs saved: 'optimal_graph_no_time.html' (No Time Restriction), 'optimal_graph_with_time.html' (With Time Restriction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 json으로 저장후 다시 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before saving to JSON, convert the data to standard Python types\n",
    "nodes_json = [{\n",
    "    \"id\": int(node[\"id\"]),  # Convert int64 to regular Python int\n",
    "    \"name\": str(node[\"name\"]),\n",
    "    \"area\": str(node[\"area\"]),\n",
    "    \"subject_id\": int(node[\"subject_id\"]),\n",
    "    \"color\": str(node[\"color\"]),\n",
    "    \"border_color\": str(node[\"border_color\"])\n",
    "} for node in nodes_json]\n",
    "\n",
    "edges_json = [{\n",
    "    \"source\": int(edge[\"source\"]),  # Convert int64 to regular Python int\n",
    "    \"target\": int(edge[\"target\"])\n",
    "} for edge in edges_json]\n",
    "\n",
    "graph_data = {\n",
    "    \"nodes\": nodes_json,\n",
    "    \"edges\": edges_json\n",
    "}\n",
    "\n",
    "# Now save to JSON\n",
    "with open('chapter_graph_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(graph_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Edges: []\n",
      "Similarity Edges: []\n"
     ]
    }
   ],
   "source": [
    "target_chapter_id = data[data[\"f_mchapter_nm\"] == \"쌓기나무\"][\"f_mchapter_id\"].values[0]\n",
    "print(\"Flow Edges:\", [(source, target) for source, target in flow_edges if source == target_chapter_id or target == target_chapter_id])\n",
    "print(\"Similarity Edges:\", [(source, target) for source, target in similarity_edges if source == target_chapter_id or target == target_chapter_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph with embedding-based clustering saved to final_chapter_graph_clustered4.html\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Neo4j 연결 설정\n",
    "neo4j_url = \"bolt://localhost:7687\"  # 네오4j 기본 URL\n",
    "neo4j_user = \"neo4j\"  # 사용자 이름\n",
    "neo4j_password = \"12345678\"  # 비밀번호 설정\n",
    "graph = Graph(neo4j_url, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "# Neo4j 데이터 초기화 (옵션: 기존 데이터 유지 시 생략 가능)\n",
    "def clear_database():\n",
    "    graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j 데이터베이스 초기화 완료.\")\n",
    "\n",
    "clear_database()  # 데이터 초기화\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess data\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^\\uAC00-\\uD7A3a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 학년 학기별 색상 매핑\n",
    "subject_colors = {\n",
    "    2212: \"#FF9999\",  # 1학년 1학기\n",
    "    2213: \"#9999FF\",  # 1학년 2학기\n",
    "    2214: \"#99FF99\",  # 2학년 1학기\n",
    "    2215: \"#FFFF99\"   # 2학년 2학기\n",
    "}\n",
    "data[\"color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Combine similarities\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 흐름 기반 엣지 생성 (학년, 학기 순서와 성취기준 순서 기반)\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    if (current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"] or\n",
    "        (current_row[\"f_subject_id\"] == next_row[\"f_subject_id\"] and\n",
    "         current_row[\"성취기준코드\"] < next_row[\"성취기준코드\"])):\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 기반 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "similarity_edges = []\n",
    "node_max_connections = {}\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= 0.95:\n",
    "            source_id = data.iloc[i][\"f_mchapter_id\"]\n",
    "            target_id = data.iloc[j][\"f_mchapter_id\"]\n",
    "            if source_id != target_id:\n",
    "                if source_id not in node_max_connections or node_max_connections[source_id][1] < final_sim[i, j]:\n",
    "                    node_max_connections[source_id] = (target_id, final_sim[i, j])\n",
    "\n",
    "for source_id, (target_id, weight) in node_max_connections.items():\n",
    "    similarity_edges.append((source_id, target_id))\n",
    "\n",
    "# 모든 관계 통합\n",
    "all_edges = list(set(flow_edges + similarity_edges))\n",
    "\n",
    "# Neo4j 업로드 함수\n",
    "def add_nodes_and_relationships(data, all_edges):\n",
    "    for _, row in data.iterrows():\n",
    "        node = Node(\n",
    "            \"Chapter\",\n",
    "            id=int(row[\"f_mchapter_id\"]),\n",
    "            name=row[\"f_mchapter_nm\"],\n",
    "            subject_id=int(row[\"f_subject_id\"]),\n",
    "            color=row[\"color\"]\n",
    "        )\n",
    "        graph.merge(node, \"Chapter\", \"id\")\n",
    "\n",
    "    for source, target in all_edges:\n",
    "        if source != target:\n",
    "            source_node = graph.nodes.match(\"Chapter\", id=int(source)).first()\n",
    "            target_node = graph.nodes.match(\"Chapter\", id=int(target)).first()\n",
    "            if source_node and target_node:\n",
    "                relationship = Relationship(source_node, \"PRECEDES\", target_node)\n",
    "                graph.merge(relationship)\n",
    "\n",
    "# Neo4j에 데이터 추가\n",
    "add_nodes_and_relationships(data, all_edges)\n",
    "print(\"Neo4j 데이터 업로드 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neo4j 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j 데이터베이스 초기화 완료.\n",
      "Neo4j 데이터 업로드 완료.\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Neo4j 연결 설정\n",
    "neo4j_url = \"bolt://localhost:7687\"  # 네오4j 기본 URL\n",
    "neo4j_user = \"neo4j\"  # 사용자 이름\n",
    "neo4j_password = \"12345678\"  # 비밀번호 설정\n",
    "graph = Graph(neo4j_url, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "# Neo4j 데이터 초기화 (기존 데이터 삭제)\n",
    "def clear_database():\n",
    "    graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"Neo4j 데이터베이스 초기화 완료.\")\n",
    "\n",
    "clear_database()\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터 컬럼 확인\n",
    "required_columns = [\n",
    "    \"f_mchapter_id\", \"f_mchapter_nm\", \"f_subject_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# 데이터 전처리\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 학년 학기별 색상 매핑\n",
    "subject_colors = {\n",
    "    2212: \"#FF9999\",  # 1학년 1학기\n",
    "    2213: \"#9999FF\",  # 1학년 2학기\n",
    "    2214: \"#99FF99\",  # 2학년 1학기\n",
    "    2215: \"#FFFF99\"   # 2학년 2학기\n",
    "}\n",
    "data[\"color\"] = data[\"f_subject_id\"].map(subject_colors)\n",
    "\n",
    "# Sentence-BERT 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# BERT와 TF-IDF 유사도 결합\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 선후행 관계 생성\n",
    "flow_edges = []\n",
    "for i in range(len(data) - 1):\n",
    "    current_row = data.iloc[i]\n",
    "    next_row = data.iloc[i + 1]\n",
    "    if (current_row[\"f_subject_id\"] < next_row[\"f_subject_id\"] or\n",
    "        (current_row[\"f_subject_id\"] == next_row[\"f_subject_id\"] and\n",
    "         current_row[\"성취기준코드\"] < next_row[\"성취기준코드\"])):\n",
    "        flow_edges.append((current_row[\"f_mchapter_id\"], next_row[\"f_mchapter_id\"]))\n",
    "\n",
    "# 유사도 관계 생성 (가장 높은 유사도 하나만 유지)\n",
    "similarity_edges = []\n",
    "for i in range(len(data)):\n",
    "    max_similarity = 0\n",
    "    best_match = None\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] > max_similarity:\n",
    "            max_similarity = final_sim[i, j]\n",
    "            best_match = data.iloc[j][\"f_mchapter_id\"]\n",
    "    if best_match:\n",
    "        similarity_edges.append((data.iloc[i][\"f_mchapter_id\"], best_match))\n",
    "\n",
    "# Neo4j 노드 및 관계 추가\n",
    "def add_nodes_and_relationships(data, flow_edges, similarity_edges):\n",
    "    for _, row in data.iterrows():\n",
    "        node = Node(\n",
    "            \"Chapter\",\n",
    "            id=int(row[\"f_mchapter_id\"]),\n",
    "            name=row[\"f_mchapter_nm\"],\n",
    "            subject_id=int(row[\"f_subject_id\"]),\n",
    "            color=row[\"color\"]\n",
    "        )\n",
    "        graph.merge(node, \"Chapter\", \"id\")\n",
    "\n",
    "    for source, target in flow_edges:\n",
    "        if source != target:\n",
    "            source_node = graph.nodes.match(\"Chapter\", id=int(source)).first()\n",
    "            target_node = graph.nodes.match(\"Chapter\", id=int(target)).first()\n",
    "            if source_node and target_node:\n",
    "                relationship = Relationship(source_node, \"PRECEDES\", target_node)\n",
    "                graph.merge(relationship)\n",
    "\n",
    "    for source, target in similarity_edges:\n",
    "        if source != target:\n",
    "            source_node = graph.nodes.match(\"Chapter\", id=int(source)).first()\n",
    "            target_node = graph.nodes.match(\"Chapter\", id=int(target)).first()\n",
    "            if source_node and target_node:\n",
    "                relationship = Relationship(source_node, \"SIMILAR_TO\", target_node)\n",
    "                graph.merge(relationship)\n",
    "\n",
    "# 데이터 추가\n",
    "add_nodes_and_relationships(data, flow_edges, similarity_edges)\n",
    "print(\"Neo4j 데이터 업로드 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final_chapter_graph2.html => nodes&edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NodeMatcher' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 노드 데이터 저장\u001b[39;00m\n\u001b[0;32m      4\u001b[0m node_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode ID\u001b[39m\u001b[38;5;124m\"\u001b[39m: [node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes()],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode Label\u001b[39m\u001b[38;5;124m\"\u001b[39m: [graph\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes()],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode Color\u001b[39m\u001b[38;5;124m\"\u001b[39m: [graph\u001b[38;5;241m.\u001b[39mnodes[node][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes()]\n\u001b[0;32m      8\u001b[0m })\n\u001b[0;32m      9\u001b[0m node_output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m node_data\u001b[38;5;241m.\u001b[39mto_csv(node_output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NodeMatcher' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 노드 데이터 저장\n",
    "node_data = pd.DataFrame({\n",
    "    \"Node ID\": [node for node in graph.nodes()],\n",
    "    \"Node Label\": [graph.nodes[node][\"label\"] for node in graph.nodes()],\n",
    "    \"Node Color\": [graph.nodes[node][\"color\"] for node in graph.nodes()]\n",
    "})\n",
    "node_output_file = \"nodes_data.csv\"\n",
    "node_data.to_csv(node_output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Node data saved to {node_output_file}\")\n",
    "\n",
    "# 엣지 데이터 저장\n",
    "edge_data = pd.DataFrame({\n",
    "    \"Source\": [edge[0] for edge in graph.edges()],\n",
    "    \"Target\": [edge[1] for edge in graph.edges()],\n",
    "    \"Edge Color\": [graph.edges[edge][\"color\"] for edge in graph.edges()],\n",
    "    \"Edge Title\": [graph.edges[edge][\"title\"] for edge in graph.edges()]\n",
    "})\n",
    "edge_output_file = \"edges_data.csv\"\n",
    "edge_data.to_csv(edge_output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Edge data saved to {edge_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 중단원이 그래프에 표시되었습니다!\n",
      "그래프 노드 개수: 94\n",
      "데이터의 고유 중단원 개수: 94\n"
     ]
    }
   ],
   "source": [
    "# 그래프에 포함된 노드 확인\n",
    "graph_nodes = set(graph.nodes)\n",
    "\n",
    "# 데이터에서 중단원명(f_mchapter_nm) 기준으로 고유 값 가져오기\n",
    "unique_chapters = set(data[\"f_mchapter_id\"].astype(str))\n",
    "\n",
    "# 그래프에 표시되지 않은 중단원 찾기\n",
    "missing_chapters = unique_chapters - graph_nodes\n",
    "\n",
    "if len(missing_chapters) == 0:\n",
    "    print(\"모든 중단원이 그래프에 표시되었습니다!\")\n",
    "else:\n",
    "    print(f\"그래프에 표시되지 않은 중단원 ID: {missing_chapters}\")\n",
    "\n",
    "# 그래프에 추가된 노드 개수와 데이터의 고유 중단원 수 비교\n",
    "print(f\"그래프 노드 개수: {len(graph_nodes)}\")\n",
    "print(f\"데이터의 고유 중단원 개수: {len(unique_chapters)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence-BERT와 TF-IDF 입력 데이터 분리:\n",
    "\n",
    "- bert_text: 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C만 사용.\n",
    "- tfidf_text: 핵심키워드_v2만 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적화된 그래프가 'final_chapter_graph.html'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\"f_mchapter_nm\", \"성취기준코드\", \"성취기준 내용\", \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess 중단원명\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "\n",
    "# 텍스트 결합\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 두 가지 유사도 가중합\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준코드 흐름 기반 관계 생성\n",
    "edges_from_code = []\n",
    "previous_chapter = None\n",
    "for _, row in data.iterrows():\n",
    "    current_chapter = row[\"f_mchapter_nm\"]\n",
    "    if previous_chapter and previous_chapter != current_chapter:\n",
    "        edges_from_code.append((previous_chapter, current_chapter))\n",
    "    previous_chapter = current_chapter\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "threshold = 0.8\n",
    "edges_from_similarity = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):  # 한 방향만 설정\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_chapter = data.iloc[i][\"f_mchapter_nm\"]\n",
    "            target_chapter = data.iloc[j][\"f_mchapter_nm\"]\n",
    "            if source_chapter != target_chapter:  # 자기 자신 제외\n",
    "                edges_from_similarity.append((source_chapter, target_chapter))\n",
    "\n",
    "# 관계 통합 (중복 제거)\n",
    "final_edges = list(set(edges_from_code + edges_from_similarity))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 중단원 노드 추가\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].unique()\n",
    "for chapter in unique_mchapters:\n",
    "    graph.add_node(\n",
    "        chapter,\n",
    "        label=chapter,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        source, target,\n",
    "        color=\"blue\",\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "\n",
    "# 노드와 엣지를 추가\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 물리적 레이아웃 설정 (노드 간 거리 확대)\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -10000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 250,\n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.5,\n",
    "    \"solver\": \"barnesHut\"\n",
    "  },\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 20\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"smooth\": {\n",
    "      \"type\": \"continuous\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# 그래프 저장\n",
    "output_file = \"final_chapter_graph.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"최적화된 그래프가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 기능이 추가된 그래프가 'final_chapter_graph_with_search.html'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load your data\n",
    "file_path = \"merged_final_data_new - 복사본.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = [\"f_mchapter_nm\", \"성취기준코드\", \"성취기준 내용\", \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# Preprocess 중단원명\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "\n",
    "# 텍스트 결합\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 두 가지 유사도 가중합\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 성취기준코드 흐름 기반 관계 생성\n",
    "edges_from_code = []\n",
    "previous_chapter = None\n",
    "for _, row in data.iterrows():\n",
    "    current_chapter = row[\"f_mchapter_nm\"]\n",
    "    if previous_chapter and previous_chapter != current_chapter:\n",
    "        edges_from_code.append((previous_chapter, current_chapter))\n",
    "    previous_chapter = current_chapter\n",
    "\n",
    "# 유사도 기반 관계 생성\n",
    "threshold = 0.8\n",
    "edges_from_similarity = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):  # 한 방향만 설정\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source_chapter = data.iloc[i][\"f_mchapter_nm\"]\n",
    "            target_chapter = data.iloc[j][\"f_mchapter_nm\"]\n",
    "            if source_chapter != target_chapter:  # 자기 자신 제외\n",
    "                edges_from_similarity.append((source_chapter, target_chapter))\n",
    "\n",
    "# 관계 통합 (중복 제거)\n",
    "final_edges = list(set(edges_from_code + edges_from_similarity))\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 중단원 노드 추가\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].unique()\n",
    "for chapter in unique_mchapters:\n",
    "    graph.add_node(\n",
    "        chapter,\n",
    "        label=chapter,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# 엣지 추가\n",
    "for source, target in final_edges:\n",
    "    graph.add_edge(\n",
    "        source, target,\n",
    "        color=\"blue\",\n",
    "        title=\"선후행 관계 및 유사도 기반 연결\"\n",
    "    )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "\n",
    "# 노드와 엣지를 추가\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 네트워크 데이터를 추출\n",
    "nodes = [{\"id\": node, \"label\": data[\"label\"], \"color\": data[\"color\"]} for node, data in graph.nodes(data=True)]\n",
    "edges = [{\"from\": u, \"to\": v} for u, v in graph.edges()]\n",
    "\n",
    "# HTML에 검색 기능 추가\n",
    "custom_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Knowledge Map</title>\n",
    "    <script type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js\"></script>\n",
    "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis-network.min.css\">\n",
    "    <style>\n",
    "        #mynetwork {{\n",
    "            width: 100%;\n",
    "            height: 90vh;\n",
    "            border: 1px solid lightgray;\n",
    "        }}\n",
    "        #search-container {{\n",
    "            margin: 10px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"search-container\">\n",
    "        <label for=\"search-input\">중단원명 검색:</label>\n",
    "        <input type=\"text\" id=\"search-input\" placeholder=\"중단원명을 입력하세요\">\n",
    "        <button onclick=\"searchNode()\">검색</button>\n",
    "        <button onclick=\"resetGraph()\">전체 보기</button>\n",
    "    </div>\n",
    "    <div id=\"mynetwork\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "        var nodes = {nodes};\n",
    "        var edges = {edges};\n",
    "        var container = document.getElementById('mynetwork');\n",
    "        var data = {{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }};\n",
    "        var options = {{\n",
    "            physics: {{\n",
    "                barnesHut: {{\n",
    "                    gravitationalConstant: -10000,\n",
    "                    centralGravity: 0.3,\n",
    "                    springLength: 250,\n",
    "                    springConstant: 0.05\n",
    "                }},\n",
    "                minVelocity: 0.5\n",
    "            }}\n",
    "        }};\n",
    "        var network = new vis.Network(container, data, options);\n",
    "\n",
    "        function searchNode() {{\n",
    "            var searchValue = document.getElementById('search-input').value.trim();\n",
    "            if (searchValue === \"\") {{\n",
    "                alert(\"검색할 중단원명을 입력하세요!\");\n",
    "                return;\n",
    "            }}\n",
    "            var connectedNodes = network.getConnectedNodes(searchValue);\n",
    "            if (connectedNodes.length === 0) {{\n",
    "                alert(\"해당 중단원명과 연결된 노드가 없습니다.\");\n",
    "                return;\n",
    "            }}\n",
    "            var filteredNodes = [searchValue].concat(connectedNodes);\n",
    "            var newNodes = nodes.filter(node => filteredNodes.includes(node.id));\n",
    "            var newEdges = edges.filter(edge => filteredNodes.includes(edge.from) && filteredNodes.includes(edge.to));\n",
    "            network.setData({{ nodes: new vis.DataSet(newNodes), edges: new vis.DataSet(newEdges) }});\n",
    "        }}\n",
    "\n",
    "        function resetGraph() {{\n",
    "            network.setData({{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }});\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 저장\n",
    "output_file = \"final_chapter_graph_with_search.html\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_html)\n",
    "\n",
    "print(f\"검색 기능이 추가된 그래프가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "화살표가 추가된 그래프가 'final_chapter_graph_with_arrows.html'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\", notebook=True)\n",
    "\n",
    "# 노드와 엣지를 추가\n",
    "net.from_nx(graph)\n",
    "\n",
    "# 화살표 활성화\n",
    "for edge in net.edges:\n",
    "    edge['arrows'] = 'to'  # 화살표가 타겟 방향으로 나타나도록 설정\n",
    "\n",
    "# HTML에 검색 기능 추가\n",
    "custom_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Knowledge Map</title>\n",
    "    <script type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js\"></script>\n",
    "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis-network.min.css\">\n",
    "    <style>\n",
    "        #mynetwork {{\n",
    "            width: 100%;\n",
    "            height: 90vh;\n",
    "            border: 1px solid lightgray;\n",
    "        }}\n",
    "        #search-container {{\n",
    "            margin: 10px;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"search-container\">\n",
    "        <label for=\"search-input\">중단원명 검색:</label>\n",
    "        <input type=\"text\" id=\"search-input\" placeholder=\"중단원명을 입력하세요\">\n",
    "        <button onclick=\"searchNode()\">검색</button>\n",
    "        <button onclick=\"resetGraph()\">전체 보기</button>\n",
    "    </div>\n",
    "    <div id=\"mynetwork\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "        var nodes = {nodes};\n",
    "        var edges = {edges};\n",
    "        var container = document.getElementById('mynetwork');\n",
    "        var data = {{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }};\n",
    "        var options = {{\n",
    "            physics: {{\n",
    "                barnesHut: {{\n",
    "                    gravitationalConstant: -10000,\n",
    "                    centralGravity: 0.3,\n",
    "                    springLength: 250,\n",
    "                    springConstant: 0.05\n",
    "                }},\n",
    "                minVelocity: 0.5\n",
    "            }},\n",
    "            edges: {{\n",
    "                arrows: {{\n",
    "                    to: {{enabled: true}}  // 화살표 방향 표시\n",
    "                }}\n",
    "            }}\n",
    "        }};\n",
    "        var network = new vis.Network(container, data, options);\n",
    "\n",
    "        function searchNode() {{\n",
    "            var searchValue = document.getElementById('search-input').value.trim();\n",
    "            if (searchValue === \"\") {{\n",
    "                alert(\"검색할 중단원명을 입력하세요!\");\n",
    "                return;\n",
    "            }}\n",
    "            var connectedNodes = network.getConnectedNodes(searchValue);\n",
    "            if (connectedNodes.length === 0) {{\n",
    "                alert(\"해당 중단원명과 연결된 노드가 없습니다.\");\n",
    "                return;\n",
    "            }}\n",
    "            var filteredNodes = [searchValue].concat(connectedNodes);\n",
    "            var newNodes = nodes.filter(node => filteredNodes.includes(node.id));\n",
    "            var newEdges = edges.filter(edge => filteredNodes.includes(edge.from) && filteredNodes.includes(edge.to));\n",
    "            network.setData({{ nodes: new vis.DataSet(newNodes), edges: new vis.DataSet(newEdges) }});\n",
    "        }}\n",
    "\n",
    "        function resetGraph() {{\n",
    "            network.setData({{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }});\n",
    "        }}\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# 저장\n",
    "output_file = \"final_chapter_graph_with_arrows.html\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_html)\n",
    "\n",
    "print(f\"화살표가 추가된 그래프가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive knowledge map saved to interactive_knowledge_map.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필수 컬럼 확인\n",
    "required_columns = [\n",
    "    \"f_lchapter_nm\", \"f_mchapter_nm\", \"f_schapter_nm\", \"f_tchapter_nm\",\n",
    "    \"성취기준코드\", \"성취기준 내용\", \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"필수 컬럼 '{col}'이(가) 데이터에 없습니다.\")\n",
    "\n",
    "# 데이터 전처리\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 최종 유사도\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "node_id_map = {}\n",
    "for index, row in data.iterrows():\n",
    "    l_id = f\"l_{row['f_lchapter_nm']}\"\n",
    "    m_id = f\"m_{row['f_mchapter_nm']}\"\n",
    "    s_id = f\"s_{row['f_schapter_nm']}\"\n",
    "    t_id = f\"t_{row['f_tchapter_nm']}\"\n",
    "\n",
    "    node_id_map[l_id] = row['f_lchapter_nm']\n",
    "    node_id_map[m_id] = row['f_mchapter_nm']\n",
    "    node_id_map[s_id] = row['f_schapter_nm']\n",
    "    node_id_map[t_id] = row['f_tchapter_nm']\n",
    "\n",
    "    graph.add_node(l_id, label=row['f_lchapter_nm'], color=\"red\", size=40)\n",
    "    graph.add_node(m_id, label=row['f_mchapter_nm'], color=\"green\", size=30)\n",
    "    graph.add_node(s_id, label=row['f_schapter_nm'], color=\"blue\", size=20)\n",
    "    graph.add_node(t_id, label=row['f_tchapter_nm'], color=\"orange\", size=15)\n",
    "\n",
    "    graph.add_edge(l_id, m_id, color=\"gray\", title=\"대단원 -> 중단원\")\n",
    "    graph.add_edge(m_id, s_id, color=\"gray\", title=\"중단원 -> 소단원\")\n",
    "    graph.add_edge(s_id, t_id, color=\"gray\", title=\"소단원 -> 토픽단원\")\n",
    "\n",
    "# 엣지 추가 (유사도 기반)\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= 0.75:\n",
    "            source = f\"m_{data.iloc[i]['f_mchapter_nm']}\"\n",
    "            target = f\"m_{data.iloc[j]['f_mchapter_nm']}\"\n",
    "            graph.add_edge(source, target, color=\"blue\", title=\"유사도 기반 연결\")\n",
    "\n",
    "# PyVis 생성\n",
    "net = Network(height=\"90vh\", width=\"100%\", directed=True)\n",
    "net.from_nx(graph)\n",
    "\n",
    "# HTML 생성\n",
    "nodes = json.dumps(net.get_nodes())\n",
    "edges = json.dumps(net.get_edges())\n",
    "\n",
    "custom_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Interactive Knowledge Map</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js\"></script>\n",
    "    <style>\n",
    "        #mynetwork {{ width: 100%; height: 90vh; border: 1px solid lightgray; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"mynetwork\"></div>\n",
    "    <script>\n",
    "        var nodes = {nodes};\n",
    "        var edges = {edges};\n",
    "        var container = document.getElementById('mynetwork');\n",
    "        var data = {{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }};\n",
    "        var options = {{\n",
    "            physics: {{\n",
    "                barnesHut: {{\n",
    "                    gravitationalConstant: -2000,\n",
    "                    centralGravity: 0.5,\n",
    "                    springLength: 100\n",
    "                }}\n",
    "            }},\n",
    "            interaction: {{ hover: true }}\n",
    "        }};\n",
    "        var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 저장\n",
    "output_file = \"interactive_knowledge_map.html\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_html)\n",
    "\n",
    "print(f\"Interactive knowledge map saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive knowledge map saved to interactive_knowledge_map.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# NaN 값 제거\n",
    "data = data.dropna(subset=[\"f_lchapter_nm\", \"f_mchapter_nm\", \"f_schapter_nm\", \"f_tchapter_nm\"])\n",
    "\n",
    "# 유사도 계산 준비\n",
    "data[\"bert_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"tfidf_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = model.encode(data[\"bert_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"tfidf_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 최종 유사도\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가\n",
    "for _, row in data.iterrows():\n",
    "    l_id = f\"l_{row['f_lchapter_nm']}\"\n",
    "    m_id = f\"m_{row['f_mchapter_nm']}\"\n",
    "    s_id = f\"s_{row['f_schapter_nm']}\"\n",
    "    t_id = f\"t_{row['f_tchapter_nm']}\"\n",
    "\n",
    "    graph.add_node(l_id, label=row['f_lchapter_nm'], color=\"red\", size=40)\n",
    "    graph.add_node(m_id, label=row['f_mchapter_nm'], color=\"green\", size=30)\n",
    "    graph.add_node(s_id, label=row['f_schapter_nm'], color=\"blue\", size=20)\n",
    "    graph.add_node(t_id, label=row['f_tchapter_nm'], color=\"orange\", size=15)\n",
    "\n",
    "    graph.add_edge(l_id, m_id, color=\"gray\", title=\"대단원 -> 중단원\")\n",
    "    graph.add_edge(m_id, s_id, color=\"gray\", title=\"중단원 -> 소단원\")\n",
    "    graph.add_edge(s_id, t_id, color=\"gray\", title=\"소단원 -> 토픽단원\")\n",
    "\n",
    "# 유사도 기반 엣지 추가\n",
    "threshold = 0.75\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):\n",
    "        if final_sim[i, j] >= threshold:\n",
    "            source = f\"m_{data.iloc[i]['f_mchapter_nm']}\"\n",
    "            target = f\"m_{data.iloc[j]['f_mchapter_nm']}\"\n",
    "            graph.add_edge(source, target, color=\"blue\", title=\"유사도 기반 연결\")\n",
    "\n",
    "# PyVis 생성\n",
    "net = Network(height=\"90vh\", width=\"100%\", directed=True)\n",
    "net.from_nx(graph)\n",
    "\n",
    "# JSON 데이터 생성\n",
    "nodes = json.dumps(net.get_nodes())\n",
    "edges = json.dumps(net.get_edges())\n",
    "\n",
    "# HTML 저장\n",
    "custom_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Interactive Knowledge Map</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js\"></script>\n",
    "    <style>#mynetwork {{ width: 100%; height: 90vh; }}</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"mynetwork\"></div>\n",
    "    <script>\n",
    "        var nodes = {nodes};\n",
    "        var edges = {edges};\n",
    "        var container = document.getElementById('mynetwork');\n",
    "        var data = {{ nodes: new vis.DataSet(nodes), edges: new vis.DataSet(edges) }};\n",
    "        var options = {{\n",
    "            physics: {{\n",
    "                barnesHut: {{\n",
    "                    gravitationalConstant: -2000,\n",
    "                    centralGravity: 0.3,\n",
    "                    springLength: 150\n",
    "                }}\n",
    "            }}\n",
    "        }};\n",
    "        var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "output_file = \"interactive_knowledge_map.html\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(custom_html)\n",
    "\n",
    "print(f\"Interactive knowledge map saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빠진 중단원명: set()\n"
     ]
    }
   ],
   "source": [
    "# 데이터에서 고유 중단원명 확인\n",
    "unique_mchapters = set(data[\"f_mchapter_nm\"].unique())\n",
    "\n",
    "# 그래프에서 사용된 노드 확인\n",
    "graph_nodes = set(graph.nodes)\n",
    "\n",
    "# 빠진 중단원명 확인\n",
    "missing_mchapters = unique_mchapters - graph_nodes\n",
    "print(f\"빠진 중단원명: {missing_mchapters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양방향 엣지: [('두 수의 크기 비교', '여러 개의 수의 크기 비교하기'), ('두 수의 크기 비교', '1000과 몇천 알아보기'), ('두 수의 크기 비교', '수의 순서 알아보기'), ('두 수의 크기 비교', '네 자리 수 알아보기'), ('두 수의 크기 비교', '두 수의 크기 비교하기'), ('두 수의 크기 비교', '뛰어 세기'), ('두 수의 크기 비교', '백과 몇백 알아보기'), ('여러 가지 모양 찾아보기', '여러 가지 모양 알아보기'), ('여러 가지 모양 찾아보기', '도형'), ('여러 가지 모양 찾아보기', '쌓기나무'), ('여러 가지 모양 알아보기', '여러 가지 모양 찾아보기'), ('여러 가지 모양 알아보기', '도형'), ('여러 가지 모양 알아보기', '쌓기나무'), ('덧셈', '뺄셈'), ('뺄셈', '덧셈'), ('길이 비교하기', '담을 수 있는 양 비교하기'), ('길이 비교하기', '높이 비교하기'), ('길이 비교하기', '키 비교하기'), ('길이 비교하기', '무게 비교하기'), ('길이 비교하기', '넓이 비교하기'), ('무게 비교하기', '길이 비교하기'), ('넓이 비교하기', '길이 비교하기'), ('담을 수 있는 양 비교하기', '길이 비교하기'), ('높이 비교하기', '길이 비교하기'), ('키 비교하기', '길이 비교하기'), ('백과 몇백 알아보기', '두 수의 크기 비교'), ('뛰어 세기', '네 자리 수 알아보기'), ('뛰어 세기', '두 수의 크기 비교하기'), ('뛰어 세기', '수의 순서 알아보기'), ('뛰어 세기', '여러 개의 수의 크기 비교하기'), ('뛰어 세기', '두 수의 크기 비교'), ('뛰어 세기', '1000과 몇천 알아보기'), ('도형', '여러 가지 모양 찾아보기'), ('도형', '여러 가지 모양 알아보기'), ('쌓기나무', '여러 가지 모양 알아보기'), ('쌓기나무', '여러 가지 모양 찾아보기'), ('수의 순서 알아보기', '두 수의 크기 비교'), ('수의 순서 알아보기', '뛰어 세기'), ('두 수의 크기 비교하기', '뛰어 세기'), ('두 수의 크기 비교하기', '두 수의 크기 비교'), ('여러 개의 수의 크기 비교하기', '뛰어 세기'), ('여러 개의 수의 크기 비교하기', '두 수의 크기 비교'), ('규칙 찾기', '규칙 찾기수'), ('규칙 찾기', '규칙 만들기2'), ('규칙 찾기', '규칙 만들기1'), ('규칙 만들기1', '규칙 찾기'), ('규칙 만들기2', '규칙 찾기'), ('규칙 찾기수', '규칙 찾기'), ('1000과 몇천 알아보기', '뛰어 세기'), ('1000과 몇천 알아보기', '두 수의 크기 비교'), ('네 자리 수 알아보기', '뛰어 세기'), ('네 자리 수 알아보기', '두 수의 크기 비교')]\n"
     ]
    }
   ],
   "source": [
    "# 모든 엣지가 한 방향인지 확인\n",
    "direction_errors = [\n",
    "    (source, target) for source, target in graph.edges\n",
    "    if graph.has_edge(target, source)  # 역방향 엣지가 있는지 확인\n",
    "]\n",
    "\n",
    "print(f\"양방향 엣지: {direction_errors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     f_subject_id  f_lchapter_id f_lchapter_nm  f_mchapter_id   f_mchapter_nm  \\\n",
      "0            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "1            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "2            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "3            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "4            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "..            ...            ...           ...            ...             ...   \n",
      "799          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "800          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "801          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "802          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "803          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "\n",
      "      성취기준코드                                            성취기준 내용  \\\n",
      "0    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "1    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "2    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "3    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "4    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "..       ...                                                ...   \n",
      "799  2수02-02               자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800  2수02-02               자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "802  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "803  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "\n",
      "                                                성취수준 A  \\\n",
      "0    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "1    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "2    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "3    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "4    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "..                                                 ...   \n",
      "799     여러 가지 규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800     여러 가지 규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "\n",
      "                                                성취수준 B  \\\n",
      "0    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "1    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "2    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "3    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "4    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "..                                                 ...   \n",
      "799           규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800           규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "\n",
      "                                                성취수준 C  f_schapter_id  \\\n",
      "0    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234963   \n",
      "1    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "2    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "3    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "4    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "..                                                 ...            ...   \n",
      "799                  주어진 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.       12235356   \n",
      "800                  주어진 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.       12235356   \n",
      "801      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235356   \n",
      "802      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235357   \n",
      "803      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235357   \n",
      "\n",
      "       f_schapter_nm  f_tchapter_id                  f_tchapter_nm  \\\n",
      "0       그림의 개수 세어 보기       12236834                  개수가 같은 것끼리 잇기   \n",
      "1    그림을 세어 1~5 알아보기       12236835           1, 2, 3, 4, 5를 쓰고 읽기   \n",
      "2    그림을 세어 1~5 알아보기       12236836          관계있는 것끼리 잇기(1~5까지의 수)   \n",
      "3    그림을 세어 1~5 알아보기       12236837  그림을 세어 알맞은 수에 ○표 하기(1~5까지의 수)   \n",
      "4    그림을 세어 1~5 알아보기       12236838          그림을 세어 수 쓰기(1~5까지의 수)   \n",
      "..               ...            ...                            ...   \n",
      "799    쌓은 모양에서 규칙 찾기       12237891            쌓기나무로 쌓은 모양에서 규칙 찾기   \n",
      "800    쌓은 모양에서 규칙 찾기       12237892                규칙을 정하여 쌓기나무 쌓기   \n",
      "801    쌓은 모양에서 규칙 찾기       12237893                 물체의 배열에서 규칙 찾기   \n",
      "802       생활에서 규칙 찾기       12237894                     달력에서 규칙 찾기   \n",
      "803       생활에서 규칙 찾기       12237895                    여러 가지 규칙 찾기   \n",
      "\n",
      "                                              핵심키워드_v2  \\\n",
      "0    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "1    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "2    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "3    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "4    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "..                                                 ...   \n",
      "799                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기   \n",
      "800                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기   \n",
      "801         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "802         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "803         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "\n",
      "                       핵심키워드_v1  \\\n",
      "0    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "1    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "2    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "3    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "4    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "..                          ...   \n",
      "799                   자기 규칙, 배열   \n",
      "800                   자기 규칙, 배열   \n",
      "801               규칙 찾기, 배열, 표현   \n",
      "802               규칙 찾기, 배열, 표현   \n",
      "803               규칙 찾기, 배열, 표현   \n",
      "\n",
      "                                             bert_text  \\\n",
      "0    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "1    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "2    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "3    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "4    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "..                                                 ...   \n",
      "799  자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다. 여러 가지 규칙을...   \n",
      "800  자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다. 여러 가지 규칙을...   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "\n",
      "                                            tfidf_text  \n",
      "0    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "1    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "2    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "3    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "4    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "..                                                 ...  \n",
      "799                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기  \n",
      "800                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기  \n",
      "801         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "802         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "803         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "\n",
      "[804 rows x 18 columns]\n",
      "     f_subject_id  f_lchapter_id f_lchapter_nm  f_mchapter_id   f_mchapter_nm  \\\n",
      "0            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "1            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "2            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "3            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "4            2212       17122834     1. 9까지의 수       14201779  1 2 3 4 5 알아보기   \n",
      "..            ...            ...           ...            ...             ...   \n",
      "799          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "800          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "801          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "802          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "803          2215       17122923      6. 규칙 찾기       14201907           규칙 찾기   \n",
      "\n",
      "      성취기준코드                                            성취기준 내용  \\\n",
      "0    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "1    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "2    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "3    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "4    2수01-01  수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "..       ...                                                ...   \n",
      "799  2수02-02               자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800  2수02-02               자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "802  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "803  2수02-01      물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다.   \n",
      "\n",
      "                                                성취수준 A  \\\n",
      "0    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "1    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "2    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "3    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "4    수가 사용되는 여러 가지 실생활 상황에서 수의 필요성을 말하고, 0과 50까지 또는...   \n",
      "..                                                 ...   \n",
      "799     여러 가지 규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800     여러 가지 규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말하고, 여러 가지 방법으로 표현할 수...   \n",
      "\n",
      "                                                성취수준 B  \\\n",
      "0    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "1    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "2    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "3    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "4    실생활에서 수가 사용되는 상황을 말하고, 10개씩 묶음과 낱개를 이용하여 0과 50...   \n",
      "..                                                 ...   \n",
      "799           규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "800           규칙을 정하고, 그 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 말, 수, 그림, 구체물 등을 이용하여...   \n",
      "\n",
      "                                                성취수준 C  f_schapter_id  \\\n",
      "0    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234963   \n",
      "1    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "2    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "3    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "4    실생활에서 수가 사용됨을 알고, 구체물을 세어 0과 50까지 또는 100까지의 수 ...       12234964   \n",
      "..                                                 ...            ...   \n",
      "799                  주어진 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.       12235356   \n",
      "800                  주어진 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다.       12235356   \n",
      "801      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235356   \n",
      "802      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235357   \n",
      "803      안내된 절차에 따라 물체, 무늬, 수 등의 간단한 배열에서 규칙을 찾을 수 있다.       12235357   \n",
      "\n",
      "       f_schapter_nm  f_tchapter_id                  f_tchapter_nm  \\\n",
      "0       그림의 개수 세어 보기       12236834                  개수가 같은 것끼리 잇기   \n",
      "1    그림을 세어 1~5 알아보기       12236835           1, 2, 3, 4, 5를 쓰고 읽기   \n",
      "2    그림을 세어 1~5 알아보기       12236836          관계있는 것끼리 잇기(1~5까지의 수)   \n",
      "3    그림을 세어 1~5 알아보기       12236837  그림을 세어 알맞은 수에 ○표 하기(1~5까지의 수)   \n",
      "4    그림을 세어 1~5 알아보기       12236838          그림을 세어 수 쓰기(1~5까지의 수)   \n",
      "..               ...            ...                            ...   \n",
      "799    쌓은 모양에서 규칙 찾기       12237891            쌓기나무로 쌓은 모양에서 규칙 찾기   \n",
      "800    쌓은 모양에서 규칙 찾기       12237892                규칙을 정하여 쌓기나무 쌓기   \n",
      "801    쌓은 모양에서 규칙 찾기       12237893                 물체의 배열에서 규칙 찾기   \n",
      "802       생활에서 규칙 찾기       12237894                     달력에서 규칙 찾기   \n",
      "803       생활에서 규칙 찾기       12237895                    여러 가지 규칙 찾기   \n",
      "\n",
      "                                              핵심키워드_v2  \\\n",
      "0    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "1    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "2    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "3    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "4    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...   \n",
      "..                                                 ...   \n",
      "799                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기   \n",
      "800                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기   \n",
      "801         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "802         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "803         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물   \n",
      "\n",
      "                       핵심키워드_v1  \\\n",
      "0    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "1    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "2    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "3    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "4    수의 필요성, 0과 100까지 수, 읽기, 쓰기   \n",
      "..                          ...   \n",
      "799                   자기 규칙, 배열   \n",
      "800                   자기 규칙, 배열   \n",
      "801               규칙 찾기, 배열, 표현   \n",
      "802               규칙 찾기, 배열, 표현   \n",
      "803               규칙 찾기, 배열, 표현   \n",
      "\n",
      "                                             bert_text  \\\n",
      "0    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "1    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "2    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "3    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "4    수의 필요성을 인식하면서 0과 100까지의 수 개념을 이해하고, 수를 세고 읽고 쓸...   \n",
      "..                                                 ...   \n",
      "799  자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다. 여러 가지 규칙을...   \n",
      "800  자신이 정한 규칙에 따라 물체, 무늬, 수 등을 배열할 수 있다. 여러 가지 규칙을...   \n",
      "801  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "802  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "803  물체, 무늬, 수 등의 배열에서 규칙을 찾아 여러 가지 방법으로 표현할 수 있다. ...   \n",
      "\n",
      "                                            tfidf_text  \n",
      "0    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "1    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "2    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "3    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "4    수의 필요성수의 필요성, 0과 100까지, 수 개념, 읽기, 쓰기, 10개씩 묶음,...  \n",
      "..                                                 ...  \n",
      "799                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기  \n",
      "800                    자신, 규칙, 물체, 무늬, 수, 배열, 정하기, 따르기  \n",
      "801         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "802         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "803         물체, 무늬, 수, 배열, 규칙, 방법, 표현, 말하기, 수, 그림, 구체물  \n",
      "\n",
      "[800 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# 중복된 성취기준코드 확인\n",
    "duplicated_codes = data[data.duplicated(subset=[\"성취기준코드\"], keep=False)]\n",
    "print(duplicated_codes)\n",
    "\n",
    "# 중복된 중단원 확인\n",
    "duplicated_chapters = data[data.duplicated(subset=[\"f_mchapter_nm\"], keep=False)]\n",
    "print(duplicated_chapters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [f_subject_id, f_lchapter_id, f_lchapter_nm, f_mchapter_id, f_mchapter_nm, 성취기준코드, 성취기준 내용, 성취수준 A, 성취수준 B, 성취수준 C, f_schapter_id, f_schapter_nm, f_tchapter_id, f_tchapter_nm, 핵심키워드_v2, 핵심키워드_v1, bert_text, tfidf_text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 양방향 관계로 나타난 노드의 성취기준코드 흐름 확인\n",
    "for source, target in direction_errors:\n",
    "    print(data[(data[\"성취기준코드\"] == source) | (data[\"성취기준코드\"] == target)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성취기준코드 두 수의 크기 비교 또는 여러 개의 수의 크기 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 1000과 몇천 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 수의 순서 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 네 자리 수 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 두 수의 크기 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교 또는 백과 몇백 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 찾아보기 또는 여러 가지 모양 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 찾아보기 또는 도형이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 찾아보기 또는 쌓기나무이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 알아보기 또는 여러 가지 모양 찾아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 알아보기 또는 도형이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 가지 모양 알아보기 또는 쌓기나무이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 덧셈 또는 뺄셈이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뺄셈 또는 덧셈이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 길이 비교하기 또는 담을 수 있는 양 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 길이 비교하기 또는 높이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 길이 비교하기 또는 키 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 길이 비교하기 또는 무게 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 길이 비교하기 또는 넓이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 무게 비교하기 또는 길이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 넓이 비교하기 또는 길이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 담을 수 있는 양 비교하기 또는 길이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 높이 비교하기 또는 길이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 키 비교하기 또는 길이 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 백과 몇백 알아보기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 네 자리 수 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 두 수의 크기 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 수의 순서 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 여러 개의 수의 크기 비교하기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 뛰어 세기 또는 1000과 몇천 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 도형 또는 여러 가지 모양 찾아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 도형 또는 여러 가지 모양 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 쌓기나무 또는 여러 가지 모양 알아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 쌓기나무 또는 여러 가지 모양 찾아보기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 수의 순서 알아보기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 수의 순서 알아보기 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교하기 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 두 수의 크기 비교하기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 개의 수의 크기 비교하기 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 여러 개의 수의 크기 비교하기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 찾기 또는 규칙 찾기수이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 찾기 또는 규칙 만들기2이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 찾기 또는 규칙 만들기1이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 만들기1 또는 규칙 찾기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 만들기2 또는 규칙 찾기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 규칙 찾기수 또는 규칙 찾기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 1000과 몇천 알아보기 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 1000과 몇천 알아보기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 네 자리 수 알아보기 또는 뛰어 세기이 데이터에 존재하지 않습니다.\n",
      "성취기준코드 네 자리 수 알아보기 또는 두 수의 크기 비교이 데이터에 존재하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# source와 target 값이 데이터에 존재하는지 확인\n",
    "for source, target in direction_errors:\n",
    "    if source not in data[\"성취기준코드\"].values or target not in data[\"성취기준코드\"].values:\n",
    "        print(f\"성취기준코드 {source} 또는 {target}이 데이터에 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 성취기준코드 열 데이터 타입 확인\n",
    "print(data[\"성취기준코드\"].dtype)\n",
    "\n",
    "# direction_errors 값 타입 확인\n",
    "print(type(direction_errors[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 코드들로 오류 파악 후 수정한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge map saved to final_chapter_graph2.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필수 컬럼 확인\n",
    "required_columns = [\"f_mchapter_nm\", \"성취기준코드\", \"성취기준 내용\", \"핵심키워드_v2\", \"성취수준 A\", \"성취수준 B\", \"성취수준 C\"]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"필수 컬럼 '{col}'이(가) 데이터에 없습니다.\")\n",
    "\n",
    "# 텍스트 정제\n",
    "data[\"f_mchapter_nm\"] = data[\"f_mchapter_nm\"].str.strip().str.replace(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", regex=True)\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"성취기준 내용\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 A\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 B\"].fillna(\"\") + \" \" +\n",
    "    data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "data[\"keyword_text\"] = data[\"핵심키워드_v2\"].fillna(\"\")\n",
    "\n",
    "# Sentence-BERT 유사도 계산\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bert_embeddings = bert_model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(bert_embeddings.cpu(), bert_embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"keyword_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 최종 유사도 계산 (가중치 적용)\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "similarity_threshold = 0.75\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가 (중단원)\n",
    "unique_mchapters = data[\"f_mchapter_nm\"].drop_duplicates().tolist()\n",
    "for chapter in unique_mchapters:\n",
    "    graph.add_node(\n",
    "        chapter,\n",
    "        label=chapter,\n",
    "        color=\"green\",\n",
    "        size=30\n",
    "    )\n",
    "\n",
    "# 유사도 기반 엣지 추가\n",
    "for i in range(len(data)):\n",
    "    for j in range(i + 1, len(data)):  # 한 방향으로만 연결\n",
    "        if final_sim[i, j] >= similarity_threshold:\n",
    "            graph.add_edge(\n",
    "                data.iloc[i][\"f_mchapter_nm\"],\n",
    "                data.iloc[j][\"f_mchapter_nm\"],\n",
    "                weight=final_sim[i, j],\n",
    "                color=\"blue\",\n",
    "                title=f\"유사도: {final_sim[i, j]:.2f}\"\n",
    "            )\n",
    "\n",
    "# 성취기준 흐름 기반 엣지 추가\n",
    "previous_chapter = None\n",
    "for _, row in data.iterrows():\n",
    "    current_chapter = row[\"f_mchapter_nm\"]\n",
    "    if previous_chapter and previous_chapter != current_chapter:\n",
    "        graph.add_edge(\n",
    "            previous_chapter, current_chapter,\n",
    "            weight=1.0,\n",
    "            color=\"gray\",\n",
    "            title=\"성취기준 흐름 기반 연결\"\n",
    "        )\n",
    "    previous_chapter = current_chapter\n",
    "\n",
    "# 자기 자신을 가리키는 루프 제거\n",
    "graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "# PyVis 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Physics 설정\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -8000,\n",
    "      \"centralGravity\": 0.3,\n",
    "      \"springLength\": 200,\n",
    "      \"springConstant\": 0.05\n",
    "    },\n",
    "    \"minVelocity\": 0.75\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# HTML 저장\n",
    "output_file = \"final_chapter_graph2.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Knowledge map saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 검사 + 성취기준 코드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f_mchapter_id / f_mchapter_nm 고유값 각 94개, 84개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 중단원명: [14201779 14201799 14201798 14201889 14201857 14201858 14201780 14201803\n",
      " 14201800 14201804 14201890 14201892 14201802 14201859 14201891 14201781\n",
      " 14201860 14201801 14201861 14201806 14201805 14201786 14201874 14201787\n",
      " 14201862 14201790 14201867 14201782 14201866 14201881 14201888 14201884\n",
      " 14201885 14201810 14201864 14201863 14201789 14201788 14201865 14201886\n",
      " 14201882 14201875 14201876 14201809 14201783 14201791 14201811 14201887\n",
      " 14201883 14201813 14201812 14201821 14201894 14201820 14201819 14201893\n",
      " 14201896 14201897 14201895 14201907 14201880 14201877 14201878 14201879\n",
      " 14201784 14201808 14201807 14201785 14201868 14201869 14201870 14201794\n",
      " 14201793 14201795 14201797 14201796 14201792 14201814 14201902 14201871\n",
      " 14201872 14201903 14201904 14201873 14201815 14201898 14201816 14201817\n",
      " 14201901 14201899 14201900 14201818 14201905 14201906]\n",
      "고유 중단원명의 개수: 94\n"
     ]
    }
   ],
   "source": [
    "# 고유 값 확인\n",
    "unique_values = data[\"f_mchapter_id\"].unique()  # 중복 제거한 고유 값\n",
    "print(\"고유 중단원명:\", unique_values)\n",
    "\n",
    "# 고유 값 개수\n",
    "unique_count = len(unique_values)\n",
    "print(f\"고유 중단원명의 개수: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_mchapter_id 고유값과 매핑된 f_mchapter_nm, 대단원 정보:\n",
      "     f_lchapter_id f_lchapter_nm  f_mchapter_id                  f_mchapter_nm\n",
      "0         17122834     1. 9까지의 수       14201779             1, 2, 3, 4, 5 알아보기\n",
      "6         17122834     1. 9까지의 수       14201780                6, 7, 8, 9 알아보기\n",
      "11        17122834     1. 9까지의 수       14201781                      9까지 수의 순서\n",
      "17        17122834     1. 9까지의 수       14201782  1만큼 더 큰 수와 1만큼 더 작은 수, 0 알아보기\n",
      "25        17122834     1. 9까지의 수       14201783                     두 수의 크기 비교\n",
      "..             ...           ...            ...                            ...\n",
      "759       17122921     4. 시각과 시간       14201903                    하루의 시간 알아보기\n",
      "766       17122921     4. 시각과 시간       14201904                        달력 알아보기\n",
      "773       17122922     5. 표와 그래프       14201905                        표로 나타내기\n",
      "782       17122922     5. 표와 그래프       14201906                      그래프로 나타내기\n",
      "791       17122923      6. 규칙 찾기       14201907                          규칙 찾기\n",
      "\n",
      "[94 rows x 4 columns]\n",
      "파일로 저장되었습니다: unique_f_mchapter_id_with_lchapter_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# f_mchapter_id 중복 제거 후 매핑된 f_mchapter_nm, f_lchapter_id, f_lchapter_nm 가져오기\n",
    "unique_data = data.drop_duplicates(subset=[\"f_mchapter_id\"])[\n",
    "    [\"f_lchapter_id\", \"f_lchapter_nm\", \"f_mchapter_id\", \"f_mchapter_nm\"]\n",
    "]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"f_mchapter_id 고유값과 매핑된 f_mchapter_nm, 대단원 정보:\")\n",
    "print(unique_data)\n",
    "\n",
    "# 파일로 저장\n",
    "output_file = \"unique_f_mchapter_id_with_lchapter_mapping.csv\"\n",
    "unique_data.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"파일로 저장되었습니다: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 f_mchapter_nm에 대한 고유한 매핑:\n",
      "    f_mchapter_id  f_mchapter_nm  f_lchapter_id f_lchapter_nm\n",
      "4        14201783     두 수의 크기 비교       17122834     1. 9까지의 수\n",
      "5        14201784  여러 가지 모양 찾아보기       17122835   2. 여러 가지 모양\n",
      "6        14201785  여러 가지 모양 알아보기       17122835   2. 여러 가지 모양\n",
      "10       14201789             덧셈       17122836     3. 덧셈과 뺄셈\n",
      "11       14201790             뺄셈       17122836     3. 덧셈과 뺄셈\n",
      "13       14201792        길이 비교하기       17122837       4. 비교하기\n",
      "23       14201802     두 수의 크기 비교       17122838    5. 50까지의 수\n",
      "26       14201805          뛰어 세기       17122839     1. 세 자리 수\n",
      "27       14201806     두 수의 크기 비교       17122839     1. 세 자리 수\n",
      "30       14201809             덧셈       17122841     3. 덧셈과 뺄셈\n",
      "31       14201810             뺄셈       17122841     3. 덧셈과 뺄셈\n",
      "35       14201814        길이 비교하기       17122842      4. 길이 재기\n",
      "54       14201868  여러 가지 모양 찾아보기       17122914     3. 모양과 시각\n",
      "55       14201869  여러 가지 모양 알아보기       17122914     3. 모양과 시각\n",
      "63       14201877          규칙 찾기       17122916      5. 규칙 찾기\n",
      "77       14201891          뛰어 세기       17122918     1. 네 자리 수\n",
      "78       14201892     두 수의 크기 비교       17122918     1. 네 자리 수\n",
      "93       14201907          규칙 찾기       17122923      6. 규칙 찾기\n",
      "결과가 파일로 저장되었습니다: duplicate_f_mchapter_nm_with_lchapter_mapping.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"unique_f_mchapter_id_with_lchapter_mapping.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 중복된 f_mchapter_nm 추출\n",
    "duplicate_names = data[data.duplicated(subset=[\"f_mchapter_nm\"], keep=False)]\n",
    "\n",
    "# 고유한 f_mchapter_id, f_mchapter_nm, f_lchapter_id, f_lchapter_nm 추출\n",
    "unique_duplicates = duplicate_names[[\"f_mchapter_id\", \"f_mchapter_nm\", \"f_lchapter_id\", \"f_lchapter_nm\"]].drop_duplicates()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"중복된 f_mchapter_nm에 대한 고유한 매핑:\")\n",
    "print(unique_duplicates)\n",
    "\n",
    "# 파일로 저장\n",
    "output_file = \"duplicate_f_mchapter_nm_with_lchapter_mapping.xlsx\"\n",
    "unique_duplicates.to_excel(output_file)\n",
    "print(f\"결과가 파일로 저장되었습니다: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 소단원까지!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to knowledge_graph_with_subchapter.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvis.network import Network\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"merged_final_data_new - 복사본.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 필수 컬럼 확인\n",
    "required_columns = [\n",
    "    \"f_mchapter_nm\", \"f_mchapter_id\", \"성취기준코드\", \"성취기준 내용\",\n",
    "    \"성취수준 A\", \"성취수준 B\", \"성취수준 C\", \"f_schapter_id\",\n",
    "    \"f_schapter_nm\", \"핵심키워드_v2\"\n",
    "]\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f\"Required column '{col}' is missing from the data\")\n",
    "\n",
    "# 데이터 전처리\n",
    "data = data.drop_duplicates(subset=[\"f_mchapter_nm\", \"성취기준 내용\", \"핵심키워드_v2\"])\n",
    "data[\"combined_text\"] = (\n",
    "    data[\"f_mchapter_nm\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"성취기준 내용\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True) +\n",
    "    \" \" +\n",
    "    data[\"핵심키워드_v2\"].str.lower().str.replace(r\"[^a-z가-힣0-9\\s]\", \"\", regex=True)\n",
    ")\n",
    "data[\"tooltip\"] = (\n",
    "    \"소단원명: \" + data[\"f_schapter_nm\"].fillna(\"N/A\") +\n",
    "    \"<br>성취수준 A: \" + data[\"성취수준 A\"].fillna(\"\") +\n",
    "    \"<br>성취수준 B: \" + data[\"성취수준 B\"].fillna(\"\") +\n",
    "    \"<br>성취수준 C: \" + data[\"성취수준 C\"].fillna(\"\")\n",
    ")\n",
    "\n",
    "# 유사도 계산\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(data[\"combined_text\"].tolist(), convert_to_tensor=True)\n",
    "bert_sim = cosine_similarity(embeddings.cpu(), embeddings.cpu())\n",
    "\n",
    "# TF-IDF 유사도 계산\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data[\"combined_text\"])\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 최종 유사도 계산\n",
    "final_sim = 0.8 * bert_sim + 0.2 * tfidf_sim\n",
    "threshold = 0.75\n",
    "\n",
    "# 그래프 생성\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# 노드 추가 (소단원 포함)\n",
    "for _, row in data.iterrows():\n",
    "    # 성취기준코드 노드\n",
    "    graph.add_node(\n",
    "        row[\"성취기준코드\"],\n",
    "        label=row[\"f_schapter_nm\"] + \" - \" + row[\"성취기준코드\"],\n",
    "        tooltip=row[\"tooltip\"],\n",
    "        color=\"red\",\n",
    "        size=20\n",
    "    )\n",
    "    # 소단원명 노드\n",
    "    if pd.notna(row[\"f_schapter_id\"]):\n",
    "        graph.add_node(\n",
    "            row[\"f_schapter_id\"],\n",
    "            label=row[\"f_schapter_nm\"],\n",
    "            tooltip=\"소단원\",\n",
    "            color=\"lightblue\",\n",
    "            size=15\n",
    "        )\n",
    "        # 소단원과 성취기준 연결\n",
    "        graph.add_edge(\n",
    "            row[\"f_schapter_id\"],\n",
    "            row[\"성취기준코드\"],\n",
    "            weight=1.0,\n",
    "            title=\"소단원 연결\"\n",
    "        )\n",
    "\n",
    "# 선후행 관계 정의\n",
    "predefined_edges = [\n",
    "    (\"2수01-01\", \"2수01-02\"),\n",
    "    (\"2수01-02\", \"2수01-03\"),\n",
    "    (\"2수01-03\", \"2수01-04\"),\n",
    "    (\"2수01-05\", \"2수01-06\"),\n",
    "    (\"2수01-06\", \"2수01-07\"),\n",
    "    (\"2수01-07\", \"2수01-08\"),\n",
    "    (\"2수01-08\", \"2수01-09\"),\n",
    "    (\"2수01-10\", \"2수01-11\"),\n",
    "    (\"2수02-01\", \"2수02-02\"),\n",
    "    (\"2수03-01\", \"2수03-02\"),\n",
    "    (\"2수03-03\", \"2수03-04\"),\n",
    "    (\"2수03-04\", \"2수03-05\"),\n",
    "    (\"2수03-06\", \"2수03-07\"),\n",
    "    (\"2수03-07\", \"2수03-08\"),\n",
    "    (\"2수03-08\", \"2수03-09\"),\n",
    "    (\"2수03-10\", \"2수03-11\"),\n",
    "    (\"2수03-11\", \"2수03-12\"),\n",
    "    (\"2수03-12\", \"2수03-13\"),\n",
    "    (\"2수04-01\", \"2수04-02\"),\n",
    "    (\"2수04-02\", \"2수04-03\")\n",
    "]\n",
    "for edge in predefined_edges:\n",
    "    graph.add_edge(edge[0], edge[1], weight=1.0, title=\"선후행 관계\")\n",
    "\n",
    "# 유사도 기반 엣지 추가\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data)):\n",
    "        if i != j and final_sim[i, j] >= threshold:\n",
    "            source = data.iloc[i][\"성취기준코드\"]\n",
    "            target = data.iloc[j][\"성취기준코드\"]\n",
    "            if not graph.has_edge(source, target):\n",
    "                graph.add_edge(\n",
    "                    source,\n",
    "                    target,\n",
    "                    weight=final_sim[i, j],\n",
    "                    title=f\"유사도: {final_sim[i, j]:.2f}\",\n",
    "                    color=\"blue\",\n",
    "                    dash=True\n",
    "                )\n",
    "\n",
    "# PyVis로 시각화\n",
    "net = Network(height=\"100vh\", width=\"100vw\", directed=True, bgcolor=\"#ffffff\", font_color=\"#000000\")\n",
    "net.from_nx(graph)\n",
    "\n",
    "# Physics 설정\n",
    "net.set_options(\"\"\"\n",
    "var options = {\n",
    "  \"physics\": {\n",
    "    \"barnesHut\": {\n",
    "      \"gravitationalConstant\": -2000,\n",
    "      \"centralGravity\": 0.4,\n",
    "      \"springLength\": 300,\n",
    "      \"springConstant\": 0.1\n",
    "    },\n",
    "    \"minVelocity\": 0.5\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# 시각화 저장\n",
    "output_file = \"knowledge_graph_with_subchapter.html\"\n",
    "net.write_html(output_file)\n",
    "print(f\"Graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
